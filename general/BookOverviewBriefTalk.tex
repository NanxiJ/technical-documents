%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[handout]{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{cool}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{bm}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usetikzlibrary{positioning}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[RL for Finance]{Foundations of Reinforcement Learning with Applications in Finance} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Ashwin Rao} % Your name
\institute[Stanford] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{Stanford University
 % Your institution for the title page
}

\date{} % Date, can be changed to a custom date

\begin{document}
\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

% \begin{frame}
% \frametitle{Overview} % Table of contents slide, comment this block out to remove it
% \tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
% \end{frame}


\begin{frame}
\frametitle{A bit about me and about my book}
\pause
\begin{itemize}[<+->]
\item Co-Founder CXScore - AI to improve Customer Experience on Apps
\item Adjunct Professor, \href{https://icme.stanford.edu/}{\underline{\textcolor{blue}{Applied Mathematics (ICME)}}}, Stanford University
\item Past: MD at Morgan Stanley, Trading Strategist at Goldman Sachs
\item Wall Street career mostly in Rates and Mortgage Derivatives
\item Educational background: Algorithms Theory and Abstract Algebra
\item I direct Stanford's \href{https://mcf.stanford.edu/}{\underline{\textcolor{blue}{Mathematical \& Computational Finance program}}}
\item Research \& Teaching in: {\em RL and it's applications in Finance \& Retail}
\item Book:  \href{https://www.routledge.com/Foundations-of-Reinforcement-Learning-with-Applications-in-Finance/Rao-Jelvis/p/book/9781032124124}{\underline{\textcolor{blue}{Foundations of RL with Applications in Finance}}}
\item Lived in Mumbai, LA, NYC, London, now settled in Palo Alto

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Key features of my book}
\pause
\begin{itemize}[<+->]
\item Book blends Theory, Modeling, Algorithms, Python, Trading problems
\item Emphasis on broader principles in Applied Math \& Software Design
\item Focus on foundations and core understanding of concepts
\item Tutorial-styled coverage, sometimes compromising rigor for intuition
\item Significant emphasis on learning by coding the details
\item 5 important financial applications covered in the book
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{AI for Dynamic Decisioning under Uncertainty}
\pause
\begin{itemize}[<+->]
\item Let's browse some terms used to characterize this branch of AI
\item {\em Stochastic}: Uncertainty in key quantities, evolving over time
\item {\em Optimization}: A well-defined metric to be maximized (``The Goal'')
\item {\em Dynamic}:  Decisions need to be a function of the changing situations
\item {\em Control}: Overpower uncertainty by persistent steering towards goal
\item Jargon overload due to confluence of Control Theory, OR and AI
\item For language clarity, let's just refer to this area as {\em Stochastic Control}
\item The core framework is called {\em Markov Decision Processes} (MDP)
\item {\em Reinforcement Learning} is a class of algorithms to solve MDPs
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{The MDP Framework}
\includegraphics[width=12cm, height=7cm]{../finance/cme241/MDP.png}
\end{frame}

\begin{frame}
\frametitle{Components of the MDP Framework}
\pause
\begin{itemize}[<+->]
\item The {\em Agent} and the {\em Environment} interact in a time-sequenced loop
\item {\em Agent} responds to [{\em State}, {\em Reward}] by taking an {\em Action}
\item {\em Environment} responds by producing next step's (random) {\em State}
\item {\em Environment} also produces a (random) scalar denoted as {\em Reward}
\item Each {\em State} is assumed to have the {\em Markov Property}
\item Goal of {\em Agent} is to maximize {\em Expected Sum} of all future {\em Reward}s
\item By controlling the ({\em Policy} : {\em State} $\rightarrow$ {\em Action}) function
\item This is a dynamic (time-sequenced control) system under uncertainty
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{How a baby learns to walk}
\includegraphics[width=13cm, height=8cm]{../finance/cme241/BabyMDP.jpg}
\end{frame}

\begin{frame}
\frametitle{Many real-world problems fit this MDP framework}
\pause
\begin{itemize}[<+->]
\item Self-driving vehicle (speed/steering to optimize safety/time)
\item Game of Chess (Boolean {\em Reward} at end of game)
\item Complex Logistical Operations (eg: movements in a Warehouse)
\item Make a humanoid robot walk/run on difficult terrains
\item Manage an investment portfolio
\item Control a power station
\item Optimal decisions during a football game
\item Strategy to win an election (high-complexity MDP)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Self-Driving Vehicle}
\includegraphics[width=13cm, height=8cm]{../finance/cme241/CarMDP.jpg}
\end{frame}

\begin{frame}
\frametitle{Why are these problems hard?}
\pause
\begin{itemize}[<+->]
\item {\em State} space can be large or complex (involving many variables)
\item Sometimes, {\em Action} space is also large or complex
\item No direct feedback on ``correct'' {\em Actions} (only feedback is {\em Reward})
\item Time-sequenced complexity ({\em Actions} influence future {\em States/Actions})
\item {\em Action}s can have delayed consequences (late {\em Reward}s)
\item {\em Agent} often doesn't know the {\em Model} of the {\em Environment}
\item ``Model'' refers to probabilities of state-transitions and rewards
\item So, {\em Agent} has to learn the {\em Model} AND solve for the Optimal {\em Policy}
\item {\em Agent} {\em Action}s need to tradeoff between ``explore'' and ``exploit''
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Dynamic Programming}
\pause
\begin{itemize}[<+->]
\item When Probabilities Model is known $\Rightarrow$ {\em Dynamic Programming} (DP)
\item DP Algorithms take advantage of knowledge of probabilities
\item So, DP Algorithms do not require interaction with the environment
\item In the Language of AI, DP is a type of {\em Planning Algorithm}
\item Why is DP not effective in practice?
\pause
\begin{itemize}[<+->]
\item Curse of Dimensionality
\item Curse of Modeling
\end{itemize}
\item Curse of Dimensionality can be partially cured with Approximate DP
\item To resolve both curses effectively, we need RL
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Reinforcement Learning}
\pause
\begin{itemize}[<+->]
\item Typically in real-world, we don't have access to a Probabilities Model
\item All we have is access to an environment serving individual transitions
\item Even if MDP model is available, model updates can be challenging
\item Often real-world models end up being too large or too complex
\item Sometimes estimating a {\em sampling model} is much more feasible
\item RL interacts with either {\em actual} or {\em simulated} environment
\item Either way, we receive {\em individual transitions} to next state and reward
\item RL is a ``trial-and-error'' approach linking {\em Actions} to {\em Rewards}
\item Try different actions \& learn what works, what doesn't
\item This is hard because actions have overlapping reward sequences
\item Also, sometimes Actions result in {\em delayed Rewards}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{RL: Learning Value Function Approximation from Samples}
\pause
\begin{itemize}[<+->]
\item RL incrementally learns and improves from transitions data
\item Deep Neural Networks used to estimate expected reward sums 
\item Big Picture: Sampling and Deep Learning estimates come together
\item RL algorithms are clever about balancing  ``explore'' versus ``exploit''
\item {\bf Promise of modern A.I. is based on success of RL algorithms}
\item Potential for automated decision-making in many industries
\item In 10-20 years: Bots that act or behave more optimal than humans
\item RL already solves various low-complexity real-world problems
\item Possibilities in Finance are endless (book covers 5 key problems)
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{P1: Dynamic Asset-Allocation and Consumption}
\pause
\begin{itemize}[<+->]
\item The broad topic is Investment Management
\item Applies to Corporations as well as Individuals
\item The two considerations are:
\pause
\begin{itemize}[<+->]
\item How to allocate money across assets in one's investment portfolio
\item How much to consume for one's needs/operations/pleasures
\end{itemize}
\item We consider the dynamic version of these dual considerations
\item Asset-Allocation and Consumption decisions at each time step
\item Asset-Allocation decisions typically deal with Risk-Reward tradeoffs
\item Consumption decisions are about spending now or later
\item Objective: Horizon-Aggregated Expected \href{https://github.com/coverdrive/technical-documents/blob/master/finance/cme241/Tour-UtilityTheory.pdf}{\underline{\textcolor{blue}{Utility of Consumption}}}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{P1: Consider the simple example of Personal Finance}
\pause
\begin{itemize}[<+->]
\item Broadly speaking, Personal Finance involves the following aspects:
\pause
\begin{itemize}[<+->]
\item Receiving Money: Salary, Bonus, Rental income, Asset Liquidation etc.
\item Consuming Money: Food, Clothes, Rent/Mortgage, Car, Vacations etc.
\item Investing Money: Savings account, Stocks, Real-estate, Gold etc.
\end{itemize}
\item Goal: Maximize lifetime-aggregated Expected Utility of Consumption
\item This can be modeled as a Markov Decision Process
\item {\em State:} Age, Asset Holdings, Asset Valuation, Career situation etc.
\item {\em Action:} Changes in Asset Holdings, Optional Consumption
\item {\em Reward:} Utility of Consumption of Money
\item {\em Model:} Career uncertainties, Asset market uncertainties
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{P2: Trading Order Book (abbrev. OB)}
\includegraphics[width=11.5cm, height=7cm]{../finance/cme241/order_book.png}
\end{frame}

\begin{frame}
\frametitle{P2: Basics of Order Book (OB)}
\pause
\begin{itemize}[<+->]
\item Buyers/Sellers express their intent to trade by submitting bids/asks
\item These are Limit Orders (LO) with a price $P$ and size $N$
\item Buy LO $(P, N)$ states willingness to buy $N$ shares at a price $\leq P$
\item Sell  LO $(P, N)$ states willingness to sell $N$ shares at a price $\geq P$
\item Order Book aggregates order sizes for each unique price
\item So we can represent with two sorted lists of (Price, Size) pairs
\item We call best bid as simply {\em Bid}, best ask as {\em Ask}, their average as {\em Mid}
\item We call {\em Ask} - {\em Bid} as {\em Spread}, {\em Worst Ask} - {\em Worst Bid} as {\em Market Depth}
\item A Market Order (MO) states intent to buy/sell $N$ shares at the {\em best possible price(s)} available on the OB at the time of MO submission
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{P2: Trading Order Book}
\includegraphics[width=11.5cm, height=7cm]{../finance/cme241/order_book.png}
\end{frame}

\begin{frame}
\frametitle{P2: Price Impact and Order Book Dynamics}
\pause
\begin{itemize}[<+->]
\item A new Sell LO $(P,N)$ potentially removes best bid prices on the OB
\item After this removal, it adds to the asks side of the OB 
\item A new Buy LO operates analogously (on the other side of the OB)
\item A Sell Market Order $N$ will remove the best bid prices on the OB
\item A Buy Market Order $N$ will remove the best ask prices on the OB
\item A large-sized MO can result in a big {\em Big-Ask Spread} - we call this the {\em Temporary Price Impact}
\item {\em Spread} typically replenished by new LOs, potentially from either side
\item Subsequent Replenishment moves {\em Bid/Ask/Mid} - we call this the {\em Permanent Price Impact}
\item Price Impact Models with OB Dynamics can be quite complex
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{P2: Optimal Trade Order Execution Problem}
\pause
\begin{itemize}[<+->]
\item The task is to sell a large number $N$ of shares
\item We are allowed to trade in $T$ discrete time steps
\item We are only allowed to submit Market Orders
\item Need to consider both {\em Temporary} and {\em Permanent} Price Impact
\item For simplicity, consider a model of just the {\em Bid Price} Dynamics
\item Goal is to maximize Expected Total Utility of Sales Proceeds
\item By breaking $N$ into appropriate chunks (timed appropriately)
\item If we sell too fast, we are likely to get poor prices
\item If we sell too slow, we risk running out of time
\item Selling slowly also leads to more uncertain proceeds (lower Utility)
\item This is a Dynamic Optimization problem
\item We can model this problem as a Markov Decision Process (MDP)
\end{itemize}
\end{frame}


\end{document}