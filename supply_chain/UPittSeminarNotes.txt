Hello, I’m pleased to be here to share and learn about one of the most important business problems we are facing in the commercial world today. The topic I’m speaking on this morning is slightly tangential but related - it’s about how to leverage machine learning to improve supply chain systems, consequently making them more robust and resilient.

As this slide indicates, I have two jobs. I’m sure all of you have heard of Stanford University but all of you might not be so familiar with Wayfair. We are an e-commerce company focused on all things Home and Living. Our revenues are about 14B annually and we strongly believe in the role of mathematics and technology in growing our business and in providing a great experience for our customers and suppliers. 

Let me start this talk with an overview of Machine learning. As this picture shows, at a high level, we think of ML in 3 of its forms. Supervised, Unsupervised and this thing called Reinforcement Learning, or RL for short, which happens to be my area of research and teaching at Stanford. So what are these strange sounding terms? You can easily look them up, but the big picture is that Supervised means given explanatory variables x, you predict y. Unsupervised means you look at the structure of the variables in x and unearth patterns, structures, maybe clusters. 

RL is a much more complex beast. It is about making Optimal Decisioning in a sequential and adaptive manner when dealing with uncertainty in  your environment. As all of you know, ML has exploded in the last decade, mainly due to the breakthroughs in the areas of natural language understanding, images and videos, and in fact in playing computer games like Chess and Go. These might seem minor applications but the techniques that have been proven successful in these fields are very adaptable to take on more challenging, more complex business problems. But ML is not something you can buy and switch on and expect to work for your business. It requires tremendous investment in one’s team, their expertise, and particularly in being extremely diligent with your data. From my own industry and academic vantage points, I’m seeing some very promising results in the two domains I’ve been involved in - The finance industry and the retail industry. 

Today we are here to discuss Supply Chain because it’s really hard and it gives all of us a solid headache. But I’m here to tell you that we have some cures or at least some pills we can take to provide relief. The list of pain points is long: We all know customer demand is really hard to predict, and now we also have immense Supply challenges. Then you have Space and Throughput constraints. The plethora of Planning problems are all difficult. Perhaps the nastiest problem in Retail is keeping track of inventory and managing through damage, theft, spoilage.  What is our happy state? To have inventory in the right place at the right time, and in the right quantity, all at low-cost. The path to Nirvana is not Traditional Operations Research methods or modern Machine Learning methods, rather it has to be a thoughtful blend of both approaches. Today I have very little time, so I will only cover two 2 game-changing applications of ML: Demand Forecasting and Inventory Replenishment.

We care about demand forecasting because it is fundamental to all kinds of decisions in Supply-Chain. Decision involving Design or Planning like Designing the Network or Transport Planning, and decisions involving operational control like Replenishment or Logistics. At the core of it, demand forecasting happens in 3 dimensions. the first dimension is to look at a collection of products we want to sell, the second dimension is a collection of locations we want to sell at, and the third dimension is the future time range during which we want to sell. Depending on the nature of the problem, we forecast by slicing at an appropriate level of thickness in each of these 3 dimensions. An example of a thin slice is to consider a single product sold at a single store on a single day. This is hard because we need to capture the local effects but we typically have little data and this thin-slice forecast will be quite error-prone. An example of a thick slice is to consider sales of an entire category of products in an entire state over the next quarter. Here we need to consider all kinds of macro-economic factors and even things like weather and of course Covid. The right approach is to forecast in a hierarchical manner starting with a thick slice and increasingly slicing finer and finer, so first capturing the macros effects and then layering on local effects. A good forecast should have the property that it not only captures structural patterns in history but also identifies changes in demand regimes.

Forecasting for a single product is not only hard due to lack of enough historical data, it is also often the wrong problem to solve. The right problem to solve is to forecast for the collection of a similar or substitutable set of products. But how do we identify substitutability in the typically vast universe of products we sell? Well we collect all kinds of data - product descriptions, product images and customer interactions with our products. We throw all this rich and diverse set of data into deep neural networks. And when we peer into these neural networks, deep inside we find encodings of our products. The beautiful thing we observe is that substitutable products have similar-looking encodings. The technical term for these encodings is Embeddings, and I have link here for a quick tutorial you can do later on Embeddings. Basically, think of embeddings as low-dimensional numerical representations and can thought of as a mathematical vector. What's important is that this compact vector of numbers captures the salient aspects of our product. In fact, it not only captures intelligence on substitutability, but also on complementarity of products. We can then use these embeddings as features in a variety of machine leaning applications. But perhaps the coolest thing about embeddings is that they make ML explainable and help us visualize due to their fascinating algebraic properties. Let me illustrate this with a picture of embeddings of books flattened into just 2 dimensions. This picture is the result of training all kinds of books simply based on the text in the books and no information on the genre of the books. You can see in this picture that the genres are automatically discovered because it clusters books of each type of genre that we humans relate to. As an example, all nonfiction books got placed here on the top right and all science fiction books got placed here on the left. I cannot share our proprietary work but embeddings for products we sell come out with a similar highly intuitive picture.

The other thing I want to say about forecasting is that it should not only capture structures and patterns from historical data, it should also quickly react to recent trends and shifts in activity. This was traditionally done with old-fashioned statistical methods we refer to as time-series analysis. But in the last 5 years, we've found that these new transformer neural networks are doing a far better job that traditional time-series methods. Here's a link to learn more, mainly you should learn about this technique we call as Self-Attention. It automatically identifies how much importance to give to old patterns and how much importance to give to recent trends. There are also great computational benefits to Transformers. So Transformers have been wildly successful in Natural Language Understanding and Computer Vision. And guess what - we've managed to port this technique to many numerical time-series applications, like predicting stock prices and of course for demand forecasting. Here's a picture of a transformer trying to predict the stock market. It was not only able to predict longer-range cycles in the stock market, but was also able to predict the finer-grained peaks and troughs. I cannot show you our proprietary work, but as you can imagine the demand forecasting problem is essentially similar in nature.

Now we come to our second problem - Inventory Replenishment, which is best understood with a very simple example. We have single product in a single store with uncertain customer demand, and we can order from a supplier who happens to carry practical infinite inventory. There's obviously a cost to place an order with the supplier, mainly due to transportation and there's a lead time. Our inventory in the store carries a holding cost and if we happen to go out-of-stock, we incur a large cost we refer to as stockout cost. So then the math problem is when is the right time to order and in qhat quantity? This simple problem has a neat little formula, thanks to good old-fashioned Operations Research. And sometimes these simple formulas work reasonably well even if our actual situation is not so simple. But when we step into a real-world situation, we have all kinds of complications - frictions, constraints, uncertainties. Supply, Space and Throughput headaches. We have to deal with price elasticity, substitutable products, worry about presentation-minimums in physical stores, deal with casepacks and irregular order frequency and uncertain lead times. And many of our Supply chain networks are multi-echelon. The list goes on and on. All of this requires a formal Decision Control framework.

That framework goes by a fancy name called Markov Decision Process. Here. at every time step, the AI Agent observes the State of the System, then performs an Action, upon which the Environment serves the Agent a new random state for the next time step and a random numerical quantity we refer to as Reward. The goal of the Ai Agent is to maximize the Expected Sum of all future Rewards. This is a time-sequenced control system experiencing uncertain outcomes. We can model not just Inventory Replenishment with this Decision Process framework, but also all kinds of other problems. In fact, this framework was originally inspired by understanding how babies learn things. Here's the Process for a baby. Here we have the baby's state, here we have the baby's action, and here we have positive or negative feedback, Reward or Cost, the baby receives.

And here we have a little sample of real-world problems that can be modeled with this MDP framework. Let's take a look at a self-driving car. The car location, velocity and sensor data constitutes the State of the system, The steering, acceleration and braking constitutes the actions the AI Agent of the car can take, the Environment comprises of the traffic, signals, road conditions, routes etc. and the reward is a complicated thing comprising of ride quality and arrival time.

Our interest is to model inventory replenishment as an MDP. The State would be the count of inventory at a store or warehouse, the actions would be the number of casepacks to order (or not order), the reward woul consist of all the costs we had talked about. Thankfully we have a wide range of algorithms to solve an MDP problem. The traditional OR methodology is the technique called Dynamic Programming and the modern technique is called Reinforcement Learning, which happens to be my area of Research and Teaching. In the real-world, we typically get hit with scale and complexity that we mathematicians like to call the Curse of DImensionality and the Curse of Modeling, which makes Dynamic Programming techniques infeasible, which is why Reinforcement Learning has recently become so popular. 

My last slide is a very quick overview of how RL works. Basically, RL incrementally learns from an ongoing stream of data on state and reward transitions in the system. In practice, we build a simulator that serves as a simulated environment. The RL methodology is essentially a sort of trial-and-error learning linking actions to rewards. If an action seems to fetch good rewards, we positive reinforce it and if an actions seems to fetch poor or negative rewards, we negatively reinforce it. Now you see how this is like how we humans or animals learn. But I want you to know that a key reason why RL is doing so well now is because of the recent advances in deep learning because RL typically uses deep neural networks to approximate functions while learning. Big Picture: Clever Sampling and Clever Function Approximation come together to overcome the two curses I refered to earlier. I might be a bit biased here, but I think RL can be a game-changer for AI in general and for Supply-Chain in particular.

