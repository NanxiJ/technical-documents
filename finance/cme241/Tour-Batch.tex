%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[handout]{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{cool}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pseudocode}
\usepackage{bm}
\usepackage{multirow}
\usepackage{physics}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\prob}{\mathcal{P}}
\newcommand{\rew}{\mathcal{R}}
\newcommand{\states}{\mathcal{S}}
\newcommand{\actions}{\mathcal{S}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\bvpi}{\bm{V}^{\pi}}
\newcommand{\bvs}{\bm{V}^*}
\newcommand{\bbpi}{\bm{B}^{\pi}}
\newcommand{\bbs}{\bm{B}^*}
\newcommand{\bv}{\bm{V}}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Batch RL Chapter]{A Guided Tour of \href{http://stanford.edu/~ashlearn/RLForFinanceBook/book.pdf}{\underline{\textcolor{yellow}{Chapter 11}}}: \\ Batch RL, Experience-Replay, DQN, LSPI, Gradient TD} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Ashwin Rao} % Your name
\institute[Stanford] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{ICME, Stanford University
 % Your institution for the title page
}

\date % Date, can be changed to a custom date

\begin{document}
\lstset{language=Python}  
\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

% \begin{frame}
% \frametitle{Overview} % Table of contents slide, comment this block out to remove it
% \tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
% \end{frame}

\begin{frame}
\frametitle{Moving on to practically sophisticated algorithms}
\begin{itemize}[<+->]
\item Let's examine the core pattern of RL Algorithms we've learnt so far
\item Experiences arrive one at a time, is used (for learning) and {\em discarded}
\item Learning is {\em incremental}, with VF update after each unit of experience
\item Are there alternative patterns we can employ? The answer is {\em Yes}
\item We highlight 2 key patterns that yield a richer range of RL Algorithms
\begin{enumerate}[<+->]
\item {\em Experience-Replay}: Store the data as it arrives, and re-use it
\item {\em Batch RL}: Learn the VF for an entire batch of data directly
\end{enumerate} 
\item Experience-Replay and Batch RL can be combined in interesting ways
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Incremental RL makes inefficient use of training data}
\begin{itemize}[<+->]
\item Incremental versus Batch RL in the context of fixed finite data
\item Let's understand the difference for the simple case of MC Prediction
\item Given fixed finite sequence of trace experiences yielding training data:
$$\mathcal{D} = [(S_i, G_i) | 1 \leq i \leq n]$$
\item Incremental MC estimates $V(s;\bm{w})$ using $\nabla_{\bm{w}} \mathcal{L}(\bm{w})$ for each data pair:
$$\mathcal{L}_{(S_i, G_i)}(\bm{w}) = \frac 1 2 \cdot (V(S_i; \bm{w}) - G_i)^2$$
$$\nabla_{\bm{w}} \mathcal{L}_{(S_i, G_i)}(\bm{w}) = (V(S_i; \bm{w}) - G_i) \cdot \nabla_{\bm{w}} V(S_i; \bm{w})$$
$$\Delta \bm{w} = \alpha \cdot (G_i - V(S_i; \bm{w})) \cdot \nabla_{\bm{w}} V(S_i; \bm{w})$$
\item $n$ updates are performed in sequence for $i = 1, 2, \ldots ,n$
\item Uses  \lstinline{update} method of \lstinline{FunctionApprox} for each data pair $(S_i, G_i)$
\item Incremental RL makes inefficient use of available training data $\mathcal{D}$
\item Essentially each data point is ``discarded'' after being used for update
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Batch MC Prediction makes efficient use of training data}
\pause
\begin{itemize}[<+->]
\item Instead we'd like to estimate the Value Function $V(s;\bm{w^*})$ such that
\begin{align*}
\bm{w^*} & = \argmin_{\bm{w}} \frac 1 n \cdot \sum_{i=1}^n \frac 1 2 \cdot (V(S_i;\bm{w}) - G_i)^2 \\
& = \argmin_{\bm{w}} \mathbb{E}_{(S,G) \sim \mathcal{D}} [\frac 1 2 \cdot (V(S; \bm{w}) - G)^2]
\end{align*}
\item This is the \lstinline{solve} method of \lstinline{FunctionApprox} on training data $\mathcal{D}$
\item This approach to RL is known as {\em Batch RL}
\item \lstinline{solve} by doing \lstinline{update}s with repeated use of available data pairs
\item Each update using random data pair $(S,G) \sim \mathcal{D}$
$$\Delta \bm{w} = \alpha \cdot (G - V(S; \bm{w})) \cdot \nabla_{\bm{w}} V(S; \bm{w})$$
\item This will ultimately converge to desired value function $V(s;\bm{w^*})$
\item Repeated use of available data  known as {\em Experience-Replay} 
\item This makes more efficient use of available training data $\mathcal{D}$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Batch TD Prediction makes efficient use of Experiences}
\pause
\begin{itemize}[<+->]
\item In Batch TD Prediction, we have experiences data $\mathcal{D}$ available as:
$$\mathcal{D} = [(S_i, R_i, S'_i) | 1 \leq i \leq n]$$
\item Where $(R_i, S'_i)$ is the pair of reward and next state from a state $S_i$
\item So, Experiences $\mathcal{D}$ in the form of finite number of atomic experiences
\item This is represented in code as an \lstinline{Iterable[TransitionStep[S]]}
\item Parameters updated with repeated use of these atomic experiences
\item Each update using random data pair $(S,R,S') \sim \mathcal{D}$
$$\Delta \bm{w} = \alpha \cdot (R + \gamma \cdot V(S'; \bm{w}) - V(S; \bm{w})) \cdot \nabla_{\bm{w}} V(S; \bm{w})$$
\item This is TD Prediction with Experience-Replay on Finite Experiences $\mathcal{D}$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Batch TD($\lambda$) Prediction}
\pause
\begin{itemize}[<+->]
\item In Batch TD($\lambda$) Prediction, given finite number of trace experiences
$$\mathcal{D} = [(S_{i,0}, R_{i,1}, S_{i,1}, R_{i,2}, S_{i,2}, \ldots, R_{i,T_i}, S_{i,T_i}) | 1 \leq i \leq n]$$
\item Parameters updated with repeated use of these trace experiences
\item Randomly pick trace experience (say indexed $i$) $\sim \mathcal{D}$
\item For trace experience $i$, parameters updated at each time step $t$:
$$\bm{E}_t = \gamma \lambda \cdot \bm{E}_{t-1} + \nabla_{\bm{w}} V(S_{i,t};\bm{w})$$
$$\Delta \bm{w} = \alpha \cdot (R_{i,t+1} + \gamma \cdot V(S_{i,t+1}; \bm{w}) - V(S_{i,t}; \bm{w})) \cdot \bm{E}_t$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The Deep Q-Networks (DQN) Control Algorithm}
\pause
DQN uses {\bf Experience-Replay} and {\bf fixed Q-learning targets}.\\
\pause
At each time $t$ for each episode:
\begin{itemize}[<+->]
\item Given state $S_t$, take action $A_t$ according to $\epsilon$-greedy policy extracted from Q-network values $Q(S_t,a;\bm{w})$
\item Given state $S_t$ and action $A_t$, obtain reward $R_{t+1}$ and next state $S_{t+1}$
\item Store atomic experience $(S_t, A_t, R_{t+1}, S_{t+1})$ in replay memory $\mathcal{D}$
\item Sample random mini-batch  of atomic experiences $(s_i,a_i,r_i,s'_i) \sim \mathcal{D}$
\item Update Q-network parameters $\bm{w}$ using Q-learning targets based on ``frozen'' parameters $\bm{w}^-$ of {\em target network}
$$\Delta \bm{w} = \alpha \cdot \sum_i (r_i + \gamma \cdot \max_{a'_i} Q(s'_i, a'_i; \bm{w}^-) - Q(s_i,a_i;\bm{w})) \cdot \nabla_{\bm{w}} Q(s_i,a_i;\bm{w})$$ 
\item $S_t \leftarrow S_{t+1}$
\end{itemize}
\pause
Parameters $\bm{w}^-$ of target network infrequently updated to values of Q-network parameters $\bm{w}$ (hence, Q-learning targets treated as ``frozen'')
\end{frame}

\begin{frame}
\frametitle{Least-Squares RL Prediction}
\pause
\begin{itemize}[<+->]
\item Batch RL Prediction for general function approximation is iterative
\item Uses Experience-Replay and Gradient Descent
\item We can solve directly (without gradient) for linear function approx
\item Define a sequence of feature functions $\phi_j: \mathcal{S} \rightarrow \mathbb{R}, j = 1, 2, \ldots, m$
\item Parameters $w$ is a weights vector $\bm{w} = (w_1, w_2, \ldots, w_m) \in \mathbb{R}^m$
\item Value Function is approximated as:
$$V(s;\bm{w}) = \sum_{j=1}^m \phi_j(s) \cdot w_j = \bm{\phi}(s)^T \cdot \bm{w}$$
where $\bm{\phi}(s) \in \mathbb{R}^m$ is the feature vector for state $s$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Least-Squares Monte-Carlo (LSMC)}
\pause
\begin{itemize}[<+->]
\item Loss function for Batch MC Prediction with data $[(S_i, G_i) | 1 \leq i \leq n]$:
$$\mathcal{L}(\bm{w}) =  \frac 1 {2n} \cdot \sum_{i=1}^n (\sum_{j=1}^m \phi_j(S_i) \cdot w_j - G_i)^2 = \frac 1 {2n} \cdot \sum_{i=1}^n (\bm{\phi}(S_i)^T \cdot \bm{w} - G_i)^2$$
\item The gradient of this Loss function is set to 0 to solve for $\bm{w}^*$
$$\sum_{i=1}^n \bm{\phi}(S_i) \cdot (\bm{\phi}(S_i)^T \cdot \bm{w^*} - G_i) = 0$$
\item $\bm{w^}*$ is solved as $\bm{A}^{-1} \cdot \bm{b}$
\item $m \times m$ Matrix $\bm{A}$ is accumulated at each data pair $(S_i, G_i)$ as:
$$ \bm{A} \leftarrow \bm{A} + \bm{\phi}(S_i) \cdot \bm{\phi}(S_i)^T \text{ (i.e., outer-product of } \bm{\phi}(S_i) \text{ with itself})$$
\item $m$-Vector $\bm{b}$ is accumulated at each data pair $(S_i, G_i)$ as:
$$\bm{b} \leftarrow \bm{b} + \bm{\phi}(S_i) \cdot G_i$$
\item Sherman-Morrison incremental inverse can be done in $O(m^2)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Least-Squares Temporal-Difference (LSTD)}
\pause
\begin{itemize}[<+->]
\item Loss func for Batch TD Prediction with data $[(S_i, R_i, S'_i) | 1 \leq i \leq n]$:
$$\mathcal{L}(\bm{w}) = \frac 1 {2n} \cdot \sum_{i=1}^n (\bm{\phi}(S_i)^T \cdot \bm{w} - (R_i + \gamma \cdot \bm{\phi}(S'_i)^T \cdot \bm{w}))^2$$
\item The semi-gradient of this Loss function is set to 0 to solve for $\bm{w}^*$
$$\sum_{i=1}^n \bm{\phi}(S_i) \cdot (\bm{\phi}(S_i)^T \cdot \bm{w^*} - (R_i + \gamma \cdot \bm{\phi}(S'_i)^T \cdot \bm{w}^*)) = 0$$
\item $\bm{w}^*$ is solved as $\bm{A}^{-1} \cdot \bm{b}$
\item $m \times m$ Matrix $\bm{A}$ is accumulated at each atomic experience $(S_i, R_i, S'_i)$:
$$ \bm{A} \leftarrow \bm{A} + \bm{\phi}(S_i) \cdot (\bm{\phi}(S_i) - \gamma \cdot \bm{\phi}(S'_i))^T \text{ (note the Outer-Product)}$$
\item $m$-Vector $\bm{b}$ is accumulated at each atomic experience $(S_i, R_i, S'_i)$:
$$\bm{b} \leftarrow \bm{b} + \bm{\phi}(S_i) \cdot R_i$$
\item Sherman-Morrison incremental inverse can be done in $O(m^2)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LSTD($\lambda$)}
\pause
\begin{itemize}[<+->]
\item Likewise, we can do LSTD($\lambda$) using Eligibility Traces
\item Denote the Eligibility Traces of atomic experience $i$ as $\bm{E}_i$
\item Note: $\bm{E}_i$ accumulates $\nabla_{\bm{w}} V(s;\bm{w}) = \bm{\phi}(s)$ in each trace experience
\item When accumulating, previous step's eligibility traces discounted by $\lambda \gamma$
$$\sum_{i=1}^n \bm{E_i} \cdot (\bm{\phi}(S_i)^T \cdot \bm{w^*} - (R_i + \gamma \cdot \bm{\phi}(S'_i)^T \cdot \bm{w}^*)) = 0$$
\item $\bm{w}^*$ is solved as $\bm{A}^{-1} \cdot \bm{b}$
\item $m \times m$ Matrix $\bm{A}$ is accumulated at each atomic experience $(S_i, R_i, S'_i)$:
$$ \bm{A} \leftarrow \bm{A} + \bm{E_i} \cdot (\bm{\phi}(S_i) - \gamma \cdot \bm{\phi}(S'_i))^T \text{ (note the Outer-Product)}$$
\item $m$-Vector $\bm{b}$ is accumulated at each atomic experience $(S_i, R_i, S'_i)$ as:
$$\bm{b} \leftarrow \bm{b} + \bm{E_i} \cdot R_i$$
\item Sherman-Morrison incremental inverse can be done in $O(m^2)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Convergence of Least Squares Prediction Algorithms}
\pause
\begin{center}
      \begin{tabular}{ccccc}
      \hline
      On/Off Policy & Algorithm & Tabular & Linear & Non-Linear \\ \hline
      \multirow{3}{*}{On-Policy} & MC & \cmark & \cmark & \cmark \\
      & {\bf LSMC} & \cmark & \cmark & - \\
      & TD & \cmark & \cmark & \xmark \\ 
      & {\bf LSTD} & \cmark & \cmark & - \\ \hline
      \multirow{3}{*}{Off-Policy} & MC & \cmark & \cmark & \cmark \\
      & {\bf LSMC} & \cmark & \xmark & - \\
      & TD & \cmark & \xmark & \xmark \\
      & {\bf LSTD} & \cmark & \xmark & - \\ \hline
      \end{tabular}
\end{center}      
\end{frame}

\begin{frame}
\frametitle{Least Squares RL Control}
\pause
\begin{itemize}[<+->]
\item To perform Least Squares RL Control, we do GPI with:
\begin{itemize}[<+->]
\item Policy Evaluation as Least-Squares Q-Value Prediction
\item Greedy (or $\epsilon$-Greedy) Policy Improvement
\end{itemize}
\item For On-Policy MC/TD Control, Q-Value Prediction (for policy $\pi$):
$$Q^{\pi}(s,a) \approx Q(s,a;\bm{w}^*) = \bm{\phi}(s,a)^T \cdot \bm{w}^*$$
\item Direct solve for $\bm{w}^*$ using experiences data generated using policy $\pi$
\item We are interested in Off-Policy Control with Least-Squares TD
\item Using the same idea as Q-Learning and with Experience-Replay
\item This technique is known as Least Squares Policy Iteration (LSPI)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Least Squares Policy Iteration (LSPI)}
\pause
\begin{itemize}[<+->]
\item Input is fixed finite data set $\mathcal{D}$ consisting of $(s,a,r,s')$ experiences
\item Goal is to determine Optimal Q-Value Linear Function Approximation
\item Each iteration of GPI starts with a deterministic target policy $\pi_D$
\item $\pi_D$ is made available from the previous iteration of GPI
\item Goal of the iteration is to solve for weights $\bm{w}^*$ to minimize:
\begin{align*}
\mathcal{L}(\bm{w}) & = \sum_i (Q(s_i,a_i; \bm{w}) - (r_i + \gamma \cdot Q(s'_i,\pi_D(s'_i); \bm{w})))^2\\
& = \sum_i (\bm{\phi}(s_i,a_i)^T \cdot \bm{w} - (r_i + \gamma \cdot \bm{\phi}(s'_i, \pi_D(s'_i))^T \cdot \bm{w}))^2
\end{align*}
\item Solved using sampled mini-batch of experiences $(s_i,a_i,r_i,s'_i)$ from $\mathcal{D}$
\item This solved $\bm{w}^*$ defines an updated Q-Value Function
\item Iteration ends by setting the target policy $\pi_D$ (for next iteration) as:
$$\pi_D(s) = \argmax_a Q(s,a; \bm{w}^*)$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Solving with weights $\bm{w}^*$ with LSTDQ}
\pause
\begin{itemize}[<+->]
\item We set the semi-gradient of $\mathcal{L}(\bm{w})$ at $\bm{w} = \bm{w}^*$ to 0
\begin{equation}
\sum_i \phi(s_i,a_i) \cdot (\bm{\phi}(s_i,a_i)^T \cdot \bm{w}^* - (r_i + \gamma \cdot \bm{\phi}(s'_i, \pi_D(s'_i))^T \cdot \bm{w}^*)) = 0
\label{eq:lspi-loss-semi-gradient}
\end{equation}
\item $\bm{w}^*$ is solved as $\bm{A}^{-1} \cdot \bm{b}$
\item $m \times m$ Matrix $\bm{A}$ is accumulated at each experience $(s_i,a_i,r_i,s'_i)$:
$$ \bm{A} \leftarrow \bm{A} + \bm{\phi}(s_i, a_i) \cdot (\bm{\phi}(s_i, a_i) - \gamma \cdot \bm{\phi}(s'_i, \pi_D(s'_i)))^T $$
\item $m$-Vector $\bm{b}$ is accumulated at each experience $(s_i,a_i,r_i,s'_i)$ as:
$$\bm{b} \leftarrow \bm{b} + \bm{\phi}(s_i, a_i) \cdot r_i$$
\item Sherman-Morrison incremental inverse can be done in $O(m^2)$
\item This least-squares solution of $\bm{w}^*$ (Prediction) is known as {\em LSTDQ}
\item GPI with LSTDQ and greedy policy improvement known as {\em LSPI}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Convergence of Control Algorithms}
\pause
\begin{center}
      \begin{tabular}{cccc}
      \hline
      Algorithm & Tabular & Linear & Non-Linear \\ \hline
      MC Control & \cmark & ( \cmark ) & \xmark \\
      SARSA & \cmark & ( \cmark ) & \xmark \\ 
      Q-Learning & \cmark & \xmark & \xmark \\
      {\bf LSPI} & \cmark & ( \cmark ) & - \\ \hline
      \end{tabular}
 \end{center}     
  \pause
  ( \cmark ) means it chatters around near-optimal Value Function   
\end{frame}


\begin{frame}
\frametitle{LSPI for Optimal Exercise of American Options}
\pause
\begin{itemize}[<+->]
\item American Option Pricing is Optimal Stopping, and hence an MDP
\item So can be tackled with Dynamic Programming or RL algorithms
\item But let us first review the mainstream approaches
\item For some American options, just price the European, eg: vanilla call
\item When payoff is not path-dependent and state dimension is not large, we can do backward induction on a binomial/trinomial tree/grid
\item Otherwise, the standard approach is \href{https://people.math.ethz.ch/~hjfurrer/teaching/LongstaffSchwartzAmericanOptionsLeastSquareMonteCarlo.pdf}{\underline{\textcolor{blue}{Longstaff-Schwartz algorithm}}}
\item Longstaff-Schwartz algorithm combines 3 ideas:
\begin{itemize}
\item Valuation based on Monte-Carlo simulation
\item Function approximation of continuation value for in-the-money states
\item Backward-recursive determination of early exercise states
\end{itemize}
\item We consider LSPI as an alternative approach for American Pricing
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LSPI as an alternative to Longstaff-Schwartz}
\pause
\begin{itemize}[<+->]
\item RL is straightforward if we clearly define the MDP
\item {\em State} is [Current Time, History of Underlying Security Prices]
\item {\em Action} is Boolean: Exercise (i.e., Stop) or Continue
\item {\em Reward} always 0, except upon Exercise ($=$ Payoff)
\item {\em State}-transitions based on Underlying Security's Risk-Neutral Process
\item Key is function approximation of state-conditioned continuation value
\item Continuation Value $\Rightarrow$ Optimal Stopping $\Rightarrow$ Option Price
\item We customize LSPI to Optimal Exercise of American Options
\item Based on \href{http://proceedings.mlr.press/v5/li09d/li09d.pdf}{\underline{\textcolor{blue}{this paper by Li, Szepesvari, Schuurmans}}}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LSPI customized for American Options Pricing}
\pause
\begin{itemize}[<+->]
\item 2 actions: $a=c$ (continue the option) and $a=e$ (exercise the option)
\item Create function approx representation for $Q(s,a)$ only for $a=c$ since we know option payoff $g(s)$ for $a=e$, i.e., $Q(s,a) = g(s)$
$$
\hat{Q}(s,a; \bm{w}) =
\begin{cases}
\bm{\phi}(s)^T \cdot \bm{w} & \text{ if } a = c \\
g(s) & \text{ if } a = e
\end{cases}
$$
for feature funcs $\bm{\phi}(\cdot) = [\phi_i(\cdot)|i = 1, \ldots, m]$ of only state \& not action
\item Each iteration of GPI starts with a deterministic target policy $\pi_D$
\item $\pi_D$ is greedy policy from previous iteration's solved $Q(s,a;\bm{w}^*)$
\item Since we learn Q-Value function for only $a=c$, behavior policy $\mu$ generating experiences data for training is a constant func $\mu(s) = c$
\item Also, for American Options, the reward for $a=c$ is 0
\item So each atomic experiences for training is of the form $(s,c,0,s')$
\item So we represent each atomic experience for training as a 2-tuple $(s,s')$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LSPI customized for American Options Pricing}
\pause
\begin{itemize}[<+->]
\item This reduces LSPI Semi-Gradient Equation \eqref{eq:lspi-loss-semi-gradient} to:
\begin{equation}
\sum_i \bm{\phi}(s_i) \cdot (\bm{\phi}(s_i)^T \cdot \bm{w}^* - \gamma \cdot \hat{Q}(s'_i, \pi_D(s'_i); \bm{w}^*)) = 0
\label{eq:customized-lspi-loss-semi-gradient}
\end{equation}
\item We need to consider two cases for the term $\hat{Q}(s'_i, \pi_D(s'_i); \bm{w}^*)$
\begin{itemize} 
\item  $C1$: If $s'_i$ is non-terminal and $\pi_D(s'_i) = c$ (i.e., $\bm{\phi}(s'_i)^T \cdot \bm{w} \geq g(s'_i)$):\\
Substitute $\bm{\phi}(s'_i)^T \cdot \bm{w}^*$ for $\hat{Q}(s'_i,\pi_D(s'_i); \bm{w}^*)$ in Equation \eqref{eq:customized-lspi-loss-semi-gradient}
\item $C2$: If $s'_i$ is a terminal state or $\pi_D(s'_i) = e$ (i.e., $g(s'_i) > \bm{\phi}(s'_i)^T \cdot \bm{w}$):\\
Substitute $g(s'_i)$ for $\hat{Q}(s'_i,\pi_D(s'_i); \bm{w}^*)$ in Equation \eqref{eq:customized-lspi-loss-semi-gradient}
\end{itemize}
\item So rewrite Equation \eqref{eq:customized-lspi-loss-semi-gradient} using indicator notation for cases $C1, C2$ as:
$$\sum_i \bm{\phi}(s_i) \cdot (\bm{\phi}(s_i)^T \cdot \bm{w}^* - \mathbb{I}_{C1} \cdot \gamma \cdot \bm{\phi}(s'_i)^T \cdot \bm{w}^*  -  \mathbb{I}_{C2} \cdot \gamma \cdot g(s'_i)) = 0$$
\item Factoring out $\bm{w}^*$, we get:
$$(\sum_i \bm{\phi}(s_i) \cdot (\bm{\phi}(s_i) - \mathbb{I}_{C1} \cdot \gamma \cdot \bm{\phi}(s'_i))^T) \cdot \bm{w}^*= \gamma \cdot \sum_i  \mathbb{I}_{C2} \cdot \bm{\phi}(s_i) \cdot g(s'_i)$$


\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LSPI customized for American Options Pricing}
\pause
\begin{itemize}[<+->]
\item This can be written in the familiar vector-matrix notation: $\bm{A} \cdot \bm{w}^* = \bm{b}$
$$\bm{A} = \sum_i \bm{\phi}(s_i) \cdot (\bm{\phi}(s_i) - \mathbb{I}_{C1} \cdot \gamma \cdot \bm{\phi}(s'_i))^T$$
$$\bm{b} = \gamma \cdot \sum_i \mathbb{I}_{C2} \cdot \bm{\phi}(s_i) \cdot  g(s'_i)$$
\item $m \times m$ Matrix $\bm{A}$ is accumulated at each atomic experience $(s_i,s'_i)$ as:
$$\bm{A} \leftarrow \bm{A} + \bm{\phi}(s_i) \cdot (\bm{\phi}(s_i) -  \mathbb{I}_{C1} \cdot \gamma \cdot \bm{\phi}(s'_i))^T$$
\item $m$-Vector $\bm{b}$ is accumulated at each atomic experience $(s_i, s'_i)$ as:
$$\bm{b} \leftarrow \bm{b} + \gamma  \cdot \mathbb{I}_{C2} \cdot \bm{\phi}(s_i) \cdot g(s'_i)$$
\item Sherman-Morrison incremental inverse of $\bm{A}$ can be done in $O(m^2)$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Feature functions}
\pause
\begin{itemize}[<+->]
\item Li, Szepesvari, Schuurmans recommend Laguerre polynomials (first 4)
\item Over $M_t = S_t/K$ where $S_t$ is underlying price and $K$ is strike
\item $\phi_0(S_t) = 1, \phi_1(S_t) = e^{-\frac {M_t} 2}, \phi_2(S_t) = e^{-\frac{M_t} 2} \cdot (1-M_t), \phi_3(S_t) = e^{-\frac{M_t} 2} \cdot (1-2M_t+M_t^2/2)$
\item They used these for Longstaff-Schwartz as well as for LSPI
\item For LSPI, we also need feature functions for time
\item They recommend $\phi_0^{(t)}(t) = sin(\frac {\pi(T-t)} {2T}), \phi_1^{(t)}(t) = \log(T-t), \phi_2^{(t)}(t) = (\frac t T)^2$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Deep Q-Learning for American Pricing}
\pause
\begin{itemize}[<+->]
\item LSPI is data-efficient/compute-efficient, but linearity is a limitation
\item Alternative is (incremental) Q-Learning with neural network approx
\item We employ the same set up as LSPI (including Experience-Replay)
$$
\hat{Q}(s,a; \bm{w}) =
\begin{cases}
f(s;\bm{w}) & \text{ if } a = c \\
g(s) & \text{ if } a = e
\end{cases}
$$
where $f(s; \bm{w})$ is the deep neural network function approximation
\item Q-Learning update for each atomic experience $(s_i,s'_i)$
$$\Delta \bm{w} = \alpha \cdot (\gamma \cdot \hat{Q}(s'_i, \pi(s'_i); \bm{w}) - f(s_i;\bm{w})) \cdot \nabla_{\bm{w}} f(s_i;\bm{w})$$
\item When $s'_i$ is a non-terminal state, the update is:
$$\Delta \bm{w} =  \alpha \cdot (\gamma \cdot \max(g(s'_i), f(s'_i;\bm{w})) - f(s_i;\bm{w})) \cdot \nabla_{\bm{w}} f(s_i;\bm{w})$$
\item When $s'_i$ is a terminal state, the update is:
$$\Delta \bm{w} = \alpha \cdot (\gamma \cdot g(s'_i) - f(s_i;\bm{w})) \cdot \nabla_{\bm{w}} f(s_i;\bm{w})$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Key Takeaways from this Chapter}
\pause
\begin{itemize}[<+->]
\item Batch RL makes efficient use of data
\item DQN uses Experience-Replay and fixed Q-learning targets, avoiding the pitfalls of time-correlation and semi-gradient
\item LSTD is a direct (gradient-free) solution of Batch TD Prediction
\item LSPI is an off-policy, experience-replay Control Algorithm using LSTDQ for Policy Evaluation
\item Optimal Exercise of American Options can be tackled with LSPI and Deep Q-Learning algorithms
\end{itemize}
\end{frame}


\end{document}
