%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[handout]{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{cool}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pseudocode}
\usepackage{bm}
\usepackage{multirow}
\usepackage{physics}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\prob}{\mathcal{P}}
\newcommand{\rew}{\mathcal{R}}
\newcommand{\states}{\mathcal{S}}
\newcommand{\actions}{\mathcal{S}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\bvpi}{\bm{V}^{\pi}}
\newcommand{\bvs}{\bm{V}^*}
\newcommand{\bbpi}{\bm{B}^{\pi}}
\newcommand{\bbs}{\bm{B}^*}
\newcommand{\bv}{\bm{V}}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[RL Control Chapter]{A Guided Tour of \href{http://stanford.edu/~ashlearn/RLForFinanceBook/book.pdf}{\underline{\textcolor{yellow}{Chapter 10}}}: \\ Reinforcement Learning for Control} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Ashwin Rao} % Your name
\institute[Stanford] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{ICME, Stanford University
 % Your institution for the title page
}

\date % Date, can be changed to a custom date

\begin{document}
\lstset{language=Python}  
\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

% \begin{frame}
% \frametitle{Overview} % Table of contents slide, comment this block out to remove it
% \tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
% \end{frame}

\begin{frame}
\frametitle{RL does not have access to a probability model}
\begin{itemize}[<+->]
\item DP/ADP assume access to probability model (knowledge of $\mathcal{P}_R$)
\item Often in real-world, we do not have access to these probabilities
\item Which means we'd need to {\em interact} with the {\em actual environment}
\item {Actual Environment} serves up individual experiences, not probabilities
\item Even if MDP model is available, model updates can be challenging
\item Often real-world models end up being too large or too complex
\item Sometimes estimating a {\em sampling model} is much more feasible
\item So RL interacts with either {\em actual} or {\em simulated} environment
\item Either way, we receive {\em individual experiences} of next state and reward
\item We saw how RL Prediction learns from individual experiences
\item Now we extend those ideas to RL Control: Learning Optimal VF
 \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Let us recall the Policy Iteration algorithm}
\includegraphics[width=12cm, height=5cm]{policy_iteration_loop.png}
\includegraphics[width=12cm, height=2cm]{policy_iteration_convergence.png}
\end{frame}

\begin{frame}
\frametitle{Policy Iteration}
\includegraphics[width=10cm, height=5cm]{vf_policy_intersecting_lines.png}
\pause
\begin{itemize}[<+->]
\item Policy Evaluation estimates $V^{\pi}$, eg: Iterative Policy Evaluation
\item Policy Improvement produces $\pi' \geq \pi$, eg: Greedy Policy Improvement
\item Policy Evaluation and Policy Improvement alternate until convergence
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The idea of Generalized Policy Iteration (GPI)}
\includegraphics[width=10cm, height=5cm]{gpi.png}
\pause
\begin{itemize}[<+->]
\item {\em Any} Policy Evaluation method, {\em Any} Policy Improvement method
\item For instance, {\em Partial} Policy Evaluation and {\em Partial} Policy Improvement
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Natural Idea: GPI with Tabular Monte-Carlo Evaluation}
\begin{itemize}[<+->]
\item Let us explore GPI with Tabular Monte-Carlo evaluation
\item So we will do Policy Evaluation with Tabular MC evaluation
\item And we will do the usual Greedy Policy Improvement
\item But Greedy Policy Improvement requires a model of MDP
$$\pi'_D(s) \leftarrow  \argmax_{a\in \mathcal{A}} \{\mathcal{R}(s,a) + \gamma \sum_{s' \in \mathcal{N}} \mathcal{P}(s,a,s') \cdot V^{\pi}(s') \}$$
\item However, it works if we were working with Action-Value Function
$$\pi'_D(s) \leftarrow \argmax_{a\in \mathcal{A}} Q^{\pi}(s,a)$$
\item This means: Policy Evaluation for Action-Value Function $Q^{\pi}(s,a)$
\item Following a policy $\pi$, update Q-value for each $(S_t,A_t)$ each episode:
$$Count(S_t,A_t) \leftarrow Count(S_t,A_t) + 1$$
$$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \frac 1 {Count(S_t, A_t)} \cdot (G_t - Q(S_t,A_t))$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{$\epsilon$-Greedy Policy Improvement}
\begin{itemize}[<+->]
\item A full Policy Evaluation with MC takes too long
\item So we typically improve policy after each episode
\item This can lead to some actions not being tried enough
\item Which can lead to premature (greedy) domination of an action
\item Which can lead to other actions getting ``locked-out''
\item Same as {\em Explore v/s Exploit} dilemma of Multi-Armed Bandit problem
\item Simple solution: Perform an $\epsilon$-Greedy Policy Improvement
\item All $|\mathcal{A}|$ actions are tried with non-zero probability (for each state)
\item Pick the greedy action with probability $1-\epsilon$
\item With probability $\epsilon$, randomly choose one of the $|\mathcal{A}|$ actions
$$\text{Stochastic Policy } \pi(s,a) = 
\begin{cases}
\frac {\epsilon} {|\mathcal{A}|} + 1 - \epsilon & \text{ if } a = \argmax_{b \in \mathcal{A}} Q(s, b) \\
\frac {\epsilon} {|\mathcal{A}|} & \text{ otherwise}
\end{cases}
$$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{$\epsilon$-Greedy improves the policy}
\pause
\begin{theorem}
For any $\epsilon$-greedy policy $\pi$, the $\epsilon$-greedy policy $\pi'$ with respect to $Q^{\pi}$ is an improvement over $\pi$, i.e., $\bm{V}^{\pi'}(s) \geq \bm{V}^{\pi}(s)$ for all $s \in \mathcal{N}$.
\end{theorem}
\pause
\begin{itemize}[<+->]
\item Applying $\bm{B}^{\pi'}$ repeatedly (starting with $\bvpi$) converges to $\bm{V}^{\pi'}$:
$$\lim_{i\rightarrow \infty} (\bm{B}^{\pi'})^i(\bvpi) = \bm{V}^{\pi'}$$
\item So the proof is complete if we prove that:
$$(\bm{B}^{\pi'})^{i+1}(\bvpi) \geq (\bm{B}^{\pi'})^i(\bvpi) \text{ for all } i = 0, 1, 2, \ldots$$
\item Non-decreasing tower of Value Functions $[(\bm{B}^{\pi'})^i(\bvpi)|i = 0, 1, 2, \ldots]$ with repeated applications of $\bm{B}^{\pi'}$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Base Case of Proof by Induction}
\pause
The base case of proof by induction is to show that $\bm{B}^{\pi'}(\bvpi)  \geq  \bm{V}^{\pi}$
\pause
\begin{align*}
\bm{B}^{\pi'}(\bvpi)(s) & = (\bm{\mathcal{R}}^{\pi'} + \gamma \cdot \bm{\mathcal{P}}^{\pi'} \cdot \bvpi)(s) \\
& = \bm{\mathcal{R}}^{\pi'}(s) + \gamma \cdot \sum_{s' \in \mathcal{S}} \bm{\mathcal{P}}^{\pi'}(s,s') \cdot \bvpi(s') \\
& = \sum_{a \in \mathcal{A}} \pi'(s, a) \cdot (\mathcal{R}(s,a) + \gamma \cdot \sum_{s' \in \mathcal{S}} \mathcal{P}(s,a,s') \cdot \bvpi(s')) \\
& = \sum_{a \in \mathcal{A}} \pi'(s, a) \cdot Q^{\pi}(s,a) \\
& = \frac {\epsilon} {|\mathcal{A}|} \cdot \sum_{a \in \mathcal{A}} Q^{\pi}(s,a) + (1 - \epsilon) \cdot \max_{a \in \mathcal{A}} Q^{\pi}(s,a) \\
& \geq \frac {\epsilon} {|\mathcal{A}|} \cdot \sum_{a \in \mathcal{A}} Q^{\pi}(s,a) + (1 - \epsilon) \cdot Q^{\pi}(s,\argmax_{a \in \mathcal{A}} \pi(s,a)) \\
& = \sum_{a \in \mathcal{A}} \pi(s,a) \cdot Q^{\pi}(s,a) = \bm{V}^{\pi}(s)
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Induction Step of Proof by Induction}
\pause
Induction step is proved by monotonicity of $\bm{B}^{\pi}$ operator (for any $\pi$):
\pause
$$\text{Monotonicity Property of } \bm{B}^{\pi}: \bm{X} \geq \bm{Y} \Rightarrow \bm{B}^{\pi}(\bm{X}) \geq \bm{B}^{\pi}(\bm{Y})$$
\pause
$$\text{So } (\bm{B}^{\pi'})^{i+1}(\bvpi) \geq (\bm{B}^{\pi'})^i(\bvpi) \Rightarrow (\bm{B}^{\pi'})^{i+2}(\bvpi) \geq (\bm{B}^{\pi'})^{i+1}(\bvpi)$$
\qed
\end{frame}


\begin{frame}
\frametitle{GLIE}
\pause
\begin{definition}
{\em Greedy in the Limit with Infinite Exploration} (GLIE):
\pause
\begin{itemize}[<+->]
\item  All state-action pairs are explored infinitely many times
$$\lim_{k \rightarrow \infty} N_k(s,a) = \infty$$
\item The policy converges to a greedy policy
$$\lim_{k\rightarrow \infty} \pi_k(s,a) = \mathbb{I}_{a = \argmax_{b \in \mathcal{A}} Q(s,b)}$$
\end{itemize}
\end{definition}
\pause
$\epsilon$-greedy can be made GLIE if $\epsilon$ is reduced as: $\epsilon_k = \frac 1 k$
\end{frame}

\begin{frame}
\frametitle{GLIE Tabular Monte-Carlo Control}
\pause
\begin{itemize}[<+->]
\item  Sample $k$-th episode using $\pi$: $\{S_0, A_0, R_1, S_1, A_1, \ldots, R_T, S_T\} \sim \pi$
\item For each state $S_t$ and action $A_t$ in episode, updates at {\em episode-end}:
$$Count(S_t,A_t) \leftarrow Count(S_t,A_t) + 1$$
$$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \frac 1 {Count(S_t, A_t)} \cdot (G_t - Q(S_t,A_t))$$
\item Improve policy at end of episode based on updated $Q$-Value function:
$$\epsilon \leftarrow \frac 1 k$$
$$\pi \leftarrow \epsilon \text{-greedy}(Q)$$
\end{itemize}
\pause
\begin{theorem}
GLIE Tabular Monte-Carlo Control converges to the Optimal Action-Value function: $Q(s,a) \rightarrow Q^*(s,a)$.
\end{theorem}
\end{frame}

\begin{frame}[fragile]
\frametitle{GLIE MC Control with Function Approximation}
\pause
\begin{lstlisting}
def glie_mc_control(
    mdp: MarkovDecisionProcess[S, A],
    states: NTStateDistribution[S],
    approx_0: QValueFunctionApprox[S, A],
    gamma: float,
    epsilon_func: Callable[[int], float],
    episode_len_tol: float = 1e-6
) -> Iterator[QValueFunctionApprox[S, A]]:
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
\frametitle{GLIE MC Control with Function Approximation}
\pause
\begin{lstlisting}
q: QValueFunctionApprox[S, A] = approx_0
p: Policy[S, A] = epsilon_greedy_policy(q, mdp, 1.0)
yield q
num_episodes: int = 0
while True:
  trace: Iterable[TransitionStep[S, A]] = \
    mdp.simulate_actions(states, p)
  num_episodes += 1
  for step in returns(trace, gamma, episode_len_tol):
    q = q.update([((step.state, step.action), step.return_)])
  p = epsilon_greedy_policy(
    q,
    mdp,
    epsilon_func(num_episodes)
  )
  yield q
\end{lstlisting}    
\end{frame}

\begin{frame}[fragile]
\frametitle{GLIE MC Control with Function Approximation}
\pause
\begin{lstlisting}    
def epsilon_greedy_policy(
    q: QValueFunctionApprox[S, A],
    mdp: MarkovDecisionProcess[S, A],
    epsilon: float = 0.0
) -> Policy[S, A]:
    def explore(s: S, mdp=mdp) -> Iterable[A]:
        return mdp.actions(NonTerminal(s))
    return RandomPolicy(Categorical(
        {UniformPolicy(explore): epsilon,
         greedy_policy_from_qvf(q, mdp.actions):
              1 - epsilon}
    ))
\end{lstlisting}
\end{frame}
    
\begin{frame}[fragile]
\frametitle{GLIE MC Control with Function Approximation}
\pause
\begin{lstlisting}
def greedy_policy_from_qvf(
    q: QValueFunctionApprox[S, A],
    actions: Callable[[NonTerminal[S]], Iterable[A]]
) -> DeterministicPolicy[S, A]:
    def optimal_action(s: S) -> A:
        _, a = q.argmax((NonTerminal(s), a)
            for a in actions(NonTerminal(s)))
        return a
    return DeterministicPolicy(optimal_action)
\end{lstlisting}    
\end{frame}




\begin{frame}
\frametitle{MC versus TD Control}
\pause
\begin{itemize}[<+->]
\item TD learning has several advantages over MC learning:
\begin{itemize}[<+->]
\item Lower variance
\item Online
\item Can work with incomplete traces or continuing traces
\item Generic interface of \lstinline{Iterable} of atomic experiences allows for serving up atomic experiences in any order (eg: atomic experience replays)
\end{itemize}
\pause
\item So use TD instead of MC in our Control loop
\pause
\begin{itemize}[<+->]
\item Apply TD to $Q(S,A)$ (instead of $V(S)$)
\item Use $\epsilon$-greedy Policy Improvement
\item Update $Q(S,A)$ after each {\em atomic experience}
\item $\epsilon$-greedy policy {\bf automatically updated} after each atomic experience
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{SARSA Algorithm}
\begin{itemize}[<+->]
\item SARSA is our first TD Control algorithm
\item Like MC Control, Policy Improvement is $\epsilon$-greedy
\item But here Policy Evaluation is with a TD target, as below:
$$\Delta \bm{w} =  \alpha \cdot (R_{t+1} + \gamma \cdot Q(S_{t+1}, A_{t+1}; \bm{w}) - Q(S_t,A_t; \bm{w})) \cdot \nabla_{\bm{w}} Q(S_t, A_t; \bm{w})$$
\item Note that parameters $\bm{w}$ are updated after each atomic experience
\item $\epsilon$-greedy policy automatically updated after each atomic experience
\item Action $A_t$ is chosen from State $S_t$ based on $\epsilon$-greedy policy
\item Action $A_{t+1}$ is chosen from State $S_{t+1}$ based on $\epsilon$-greedy policy
\item Unlike MC Control, trace experience is incrementally generated
\item Note: Instead of $\epsilon$-greedy, we could employ a more sophisticated exploratory policy derived from $Q$-value function ($\epsilon$-greedy is just our default simple exploratory policy derived from $Q$-value function)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{SARSA Visualization}
\centerline{\includegraphics[width=2cm, height=8cm]{sarsa.png}}
\end{frame}


\begin{frame}
\frametitle{Convergence of Tabular SARSA}
\pause
\begin{theorem}
Tabular SARSA converges to the Optimal Action-Value function, $Q(s,a) \rightarrow Q^*(s,a)$, under the following conditions:
\pause
\begin{itemize}[<+->]
\item GLIE sequence of policies $\pi_t(s,a)$
\item Robbins-Monro sequence of step-sizes $\alpha_t$:
$$\sum_{t=1}^{\infty} \alpha_t = \infty$$
$$\sum_{t=1}^{\infty} \alpha_t^2 < \infty$$
\end{itemize}
\end{theorem}
\end{frame}


\begin{frame}
\frametitle{$n$-step SARSA}
\pause
\begin{itemize}[<+->]
\item SARSA bootstraps the $Q$-Value Function with update:
$$\Delta \bm{w} =  \alpha \cdot (R_{t+1} + \gamma \cdot Q(S_{t+1}, A_{t+1}; \bm{w}) - Q(S_t,A_t; \bm{w})) \cdot \nabla_{\bm{w}} Q(S_t, A_t; \bm{w})$$
\item So it's natural to extend this to bootstrapping with 2 steps ahead:
$$\alpha \cdot (R_{t+1} + \gamma \cdot R_{t+2} + \gamma^2 \cdot Q(S_{t+2}, A_{t+2}; \bm{w}) - Q(S_t,A_t; \bm{w})) \cdot \nabla_{\bm{w}} Q(S_t, A_t; \bm{w})$$
\item Generalize to bootstrapping with $n \geq 1$ time steps ahead:
$$\Delta \bm{w} =  \alpha \cdot (G_{t,n} - Q(S_t,A_t; \bm{w})) \cdot \nabla_{\bm{w}} Q(S_t, A_t; \bm{w})$$
\item $G_{t,n}$ (call it $n$-step bootstrapped return) is defined as:
\begin{align*}
G_{t,n} & = \sum_{i=t+1}^{t+n} \gamma^{i-t-1} \cdot R_i  + \gamma^n \cdot Q(S_{t+n}, A_{t+n}; \bm{w}) \\
& = R_{t+1} + \gamma \cdot R_{t+2} + \ldots + \gamma^{n-1} \cdot R_{t+n} + \gamma^n \cdot Q(S_{t+n}, A_{t+n}; \bm{w})
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{$\lambda$-Return SARSA}
\pause
\begin{itemize}[<+->]
\item Instead of $G_{t,n}$, a valid target is a weighted-average target:
$$\sum_{n=1}^N u_n \cdot G_{t,n} + u \cdot G_t \text{ where } u + \sum_{n=1}^N u_n = 1$$
\item Any of the $u_n$ or $u$ can be 0, as long as they all sum up to 1
\item The $\lambda$-Return target is a special case of weights $u_n$ and $u$
$$u_n = (1 - \lambda) \cdot \lambda^{n-1} \text{ for all } n = 1, \ldots, T-t-1$$
$$u_n = 0 \text{ for all } n \geq T-t \text{ and } u = \lambda^{T-t-1}$$
\item We denote the $\lambda$-Return target as $G_t^{(\lambda)}$, defined as:
$$G_t^{(\lambda)} = (1-\lambda) \cdot \sum_{n=1}^{T-t-1} \lambda^{n-1} \cdot G_{t,n} + \lambda^{T-t-1} \cdot G_t$$
$$\Delta \bm{w} = \alpha \cdot (G_t^{(\lambda)} - Q(S_t, A_t; \bm{w})) \cdot \nabla_{\bm{w}} Q(S_t, A_t; \bm{w})$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{SARSA($\lambda$)}
\pause
\begin{itemize}[<+->]
\item $\lambda$ can be tuned from SARSA ($\lambda=0$) to MC Control ($\lambda=1$)
\item Note that for $\lambda > 0$, $\lambda$-Return SARSA is an Offline Algorithm
\item SARSA($\lambda$) is online ``version'' of $\lambda$-Return SARSA
\item Similar to TD($\lambda$) for Prediction, SARSA($\lambda$) uses Eligibility Traces
 \item Eligibility Traces $\bm{E}_t$ for a given trace experience at time $t$ defined as:
$$\bm{E}_0 = \nabla_{\bm{w}} V(S_0; \bm{w})$$
$$\bm{E}_t = \gamma \lambda \cdot \bm{E}_{t-1} + \nabla_{\bm{w}} Q(S_t,A_t;\bm{w})$$
\item SARSA($\lambda$) performs following update at each time step $t$:
$$\Delta \bm{w} = \alpha \cdot (R_{t+1} + \gamma \cdot Q(S_{t+1},A_{t+1};\bm{w}) - Q(S_t,A_t;\bm{w})) \cdot \bm{E}_t$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Off-Policy Learning}
\pause
\begin{itemize}[<+->]
\item Tension in Control Algorithms: Wanting to learn Q-Values contingent on {\em subsequent optimal behavior} versus wanting to explore all actions
\item So separate these concerns into two different policies 
\item Estimate VF for {\em target policy} $\pi$  while following {\em behavior policy} $\mu$
$$\{S_0, A_0, R_1, S_1, A_1, \ldots, R_T, S_T\} \sim \mu$$
\item Behavior Policy meant to explore to collect data for all actions
\item Target Policy is the policy to learn (driving towards Optimal Policy)
\item Why is this important?
\begin{itemize}[<+->]
\item Learning from observing  humans or other agents
\item Re-use experience generated from  old policies  $\pi_1, \pi_2, \ldots, \pi_{t-1}$
\item Learn about {\em optimal} policy while following {\em exploratory} policy
\item Learn about {\em multiple} policies while following {\em one} policy
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Off-Policy Control with Q-Learning}
\pause
\begin{itemize}[<+->]
\item Q-Learning performs Off-Policy learning of action-values $Q(s,a;\bm{w})$
\item The current action is chosen using behavior policy: $A_t \sim \mu(S_t,\cdot)$
\item The next action is chosen using target policy: $A' \sim \pi(S_t, \cdot)$
\item Update $Q(S_t, A_t;\bm{w})$ towards value of {\em targeted action}
$$\bm{w} = \alpha \cdot (R_{t+1} + \gamma \cdot Q(S_{t+1}, A'; \bm{w}) - Q(S_t, A_t; \bm{w})) \cdot \nabla_{\bm{w}} Q(S_t, A_t; \bm{w})$$ 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Q-Learning}
\pause
\begin{itemize}[<+->]
\item We now allow both behavior and target policies to {\bf improve}
\item The target (deterministic) policy $\pi_D$ is {\bf greedy} w.r.t $Q$-Value Function
$$\pi_D(S_{t+1}) = \argmax_{a' \in \mathcal{A}} Q(S_{t+1}, a'; \bm{w})$$
\item The behavior policy $\mu$ is also improving, eg: $\epsilon$-greedy w.r.t. $Q$
\item The $Q$-learning target then simplifies to:
\begin{align*}
& R_{t+1} + \gamma \cdot Q(S_{t+1}, A'; \bm{w}) \\
= & R_{t+1} + \gamma \cdot Q(S_{t+1}, \argmax_{a' \in \mathcal{A}} Q(S_{t+1}, a'; \bm{w})) \\
= & R_{t+1} + \gamma \cdot \max_{a' \in \mathcal{A}} Q(S_{t+1}, a'; \bm{w})
\end{align*}
\item Thus the update to $Q$-Value Function after each atomic experience is:
$$\bm{w} = \alpha \cdot (R_{t+1} + \gamma \cdot \max_{a' \in \mathcal{A}} Q(S_{t+1}, a'; \bm{w}) - Q(S_t, A_t; \bm{w})) \cdot \nabla_{\bm{w}} Q(S_t,A_t; \bm{w})$$
\item Tabular convergence proofs require infinite exploration of all (s, a) pairs \& appropriate stochastic approximation conditions for step sizes.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Q-Learning Control Visualization}
\centerline{\includegraphics[width=4cm, height=8cm]{q_learning.png}}
\end{frame}



\begin{frame}
\frametitle{Importance Sampling for Off-Policy Learning}
\pause
\begin{itemize}[<+->]
\item Importance Sampling refers to methods to estimate properties of a distribution $P$, given access to samples from a different distribution $Q$
\item We can calculate $\mathbb{E}_{X\sim P}[f(X)]$ given samples from $Q$ as follows:
\begin{align*}
\mathbb{E}_{X\sim P}[f(X)] & = \sum P(X) \cdot f(X) \\
& = \sum Q(X) \cdot \frac {P(X)} {Q(X)} \cdot f(X) \\
& = \mathbb{E}_{X \sim Q}[\frac {P(X)} {Q(X)} \cdot f(X)]
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Importance Sampling for Off-Policy Monte-Carlo}
\pause
\begin{itemize}[<+->]
\item Use returns generated from $\mu$ to estimate Value Function for $\pi$
\item Weight return $G_t$ according to similarity between policies
\item Multiply importance sampling corrections along whole episode
$$G_t^{\pi / \mu} = \frac {\pi(S_t, A_t)} {\mu(S_t, A_t)} \cdot  \frac {\pi(S_{t+1}, A_{t+1})} {\mu(S_{t+1}, A_{t+1})} \cdots \frac {\pi(S_{T-1}, A_{T-1})} {\mu(S_{T-1}, A_{T-1})} \cdot G_t$$
\item Update value towards {\em corrected} return
$$V(S_t) \leftarrow V(S_t) + \alpha \cdot (G_t^{\pi / \mu} - V(S_t))$$
\item Likewise for $Q$-Value Function for MC Control
\item Note: We cannot use this method if $\mu$ is zero when $\pi$ is non-zero
\item Importance sampling can dramatically increase variance
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Importance Sampling for Off-Policy Temporal-Difference}
\pause
\begin{itemize}[<+->]
\item  Use TD targets generated from $\mu$ to evaluate Value Function for $\pi$
\item Weight TD target $R + \gamma \cdot V(S')$ with importance sampling
\item Here we only need a single importance sampling correction
$$V(S_t) \leftarrow V(S_t) + \alpha \cdot ( \frac {\pi(S_t, A_t)} {\mu(S_t, A_t)} \cdot (R_{t+1} + \gamma \cdot V(S_{t+1})) - V(S_t))$$
\item Likewise for $Q$-Value Function for TD Control
\item This has much lower variance than MC importance sampling
\item Policies only need to be similar over a single step
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Relationship between DP and TD}
\pause
      \begin{tabular}{c|c|c}
      & {\bf Full Backup (DP)} & {\bf Sample Backup (TD)} \\ \hline
       Bellman Expectation & \includegraphics[width=3.0cm, height=1.9cm]{mdp_bellman_policy_tree_vv.png}  & \includegraphics[width=0.5cm, height=1.9cm]{td.png} \\
               Equation for $V^{\pi}(s)$ & Iterative Policy Evaluation & TD Learning \\ \hline
        Bellman Expectation & \includegraphics[width=3.0cm, height=1.9cm]{mdp_bellman_policy_tree_qq.png} & \includegraphics[width=0.5cm, height=1.9cm]{sarsa.png} \\
               Equation for $Q^{\pi}(s, a)$ & Q-Policy Iteration & SARSA \\ \hline
          Bellman Optimality & \includegraphics[width=3.0cm, height=1.9cm]{mdp_bellman_opt_tree_qq.png} & \includegraphics[width=1.5cm, height=1.9cm]{q_learning.png} \\
               Equation for $Q^*(s, a)$ & Q-Value Iteration & Q-Learning \\         
      \end{tabular}  
\end{frame}

\begin{frame}
\frametitle{Relationship between DP and TD}
\pause
\begin{center}
      \begin{tabular}{c|c}
      {\bf Full Backup (DP)} & {\bf Sample Backup (TD)} \\ \hline
       Iterative Policy Evaluation: $V(S)$ update & TD Learning: $V(S)$ update \\
       $\mathbb{E}[R + \gamma V(S')|S]$ & sample $R + \gamma V(S')$ \\ \hline
      Q-Policy Iteration: $Q(S,A)$ update & SARSA: $Q(S,A)$ update \\
       $\mathbb{E}[R + \gamma Q(S',A')|S,A]$ & sample $R + \gamma Q(S',A')$ \\ \hline
       Q-Value Iteration: $Q(S,A)$ update & Q-Learning: $Q(S,A)$ update \\
       $\mathbb{E}[R + \gamma \max_{a'} Q(S',a')|S,A]$ & sample $R + \gamma \max_{a' } Q(S',a')$ \\ \hline
      \end{tabular} 
\end{center}      
\end{frame}

\begin{frame}
\frametitle{Convergence of Prediction Algorithms}
\pause
\begin{center}
      \begin{tabular}{ccccc}
      \hline
      On/Off Policy & Algorithm & Tabular & Linear & Non-Linear \\ \hline
      \multirow{3}{*}{On-Policy} & MC & \cmark & \cmark & \cmark \\
      & TD(0) & \cmark & \cmark & \xmark \\
      & TD($\lambda$) & \cmark & \cmark & \xmark \\ \hline
      \multirow{3}{*}{Off-Policy} & MC & \cmark & \cmark & \cmark \\
      & TD(0) & \cmark & \xmark & \xmark \\
      & TD($\lambda$) & \cmark & \xmark & \xmark \\ \hline
      \end{tabular}
\end{center}      
\pause
Deadly Triad := [Bootstrapping, Function Approximation, Off-Policy]
\end{frame}

\begin{frame}
\frametitle{Gradient Temporal-Difference Learning}
\pause
\begin{itemize}[<+->]
\item TD does not follow the gradient of {\em any} objective function
\item This is why TD can diverge:
\begin{itemize}[<+->]
\item when running off-policy, or
\item when using non-linear function approximation
\end{itemize}
\item {\bf Gradient TD} follows true gradient of projected Bellman Error
\end{itemize}
\pause
\begin{center}
      \begin{tabular}{ccccc}
      \hline
      On/Off Policy & Algorithm & Tabular & Linear & Non-Linear \\ \hline
      \multirow{4}{*}{On-Policy} & MC & \cmark & \cmark & \cmark \\
      & TD & \cmark & \cmark & \xmark \\
      & {\bf Gradient TD} & {\bf \cmark} & {\bf \cmark} & {\bf \cmark} \\ \hline
      \multirow{4}{*}{Off-Policy} & MC & \cmark & \cmark & \cmark \\
      & TD & \cmark & \xmark & \xmark \\
      & {\bf Gradient TD} & {\bf \cmark} & {\bf \cmark} & {\bf \cmark} \\ \hline
      \end{tabular} 
\end{center}      
\end{frame}

\begin{frame}
\frametitle{Convergence of Control Algorithms}
\pause
\begin{center}
      \begin{tabular}{cccc}
      \hline
      Algorithm & Tabular & Linear & Non-Linear \\ \hline
      MC Control & \cmark & ( \cmark ) & \xmark \\
      SARSA & \cmark & ( \cmark ) & \xmark \\ 
      Q-Learning & \cmark & \xmark & \xmark \\
      {\bf Gradient Q-Learning} & \cmark & \cmark & \xmark \\ \hline
      \end{tabular}
 \end{center}     
  \pause
  ( \cmark ) means it chatters around near-optimal Value Function   
\end{frame}

\begin{frame}
\frametitle{Key Takeaways from this Chapter}
\pause
\begin{itemize}[<+->]
\item RL Control is based on the idea of Generalized Policy Iteration (GPI)
\begin{itemize}[<+->]
\item Policy Evaluation with $Q$-Value Function (instead of $V$)
\item Improved Policy needs to be exploratory, eg: $\epsilon$-greedy
\end{itemize}
\item On-Policy versus Off-Policy
\item Deadly Triad := [Bootstrapping, Function Approximation, Off-Policy]
\end{itemize}
\end{frame}


\end{document}