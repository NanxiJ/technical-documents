
\documentclass[12pt]{amsart}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} % Allows including images
\usepackage{cool}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{listings}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{pseudocode}
\usepackage{MnSymbol,wasysym}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} 
\newgeometry{vmargin={20mm}, hmargin={14mm,18mm}}

\DeclareMathOperator*{\argmax}{arg\,max}

% See the ``Article customise'' template for come common customisations

\title{RL for Optimal Exercise of American Options}
\author{Ashwin Rao (Stanford University)}
\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\maketitle

In this technical note, we explain how to solve the problem of Optimal Exercise of American Options using Reinforcement Learning. We start by showing how to solve this problem with a simple linear-approximation RL algorithm known as Least Squares Policy Iteration (LSPI). Our coverage is based on \href{http://proceedings.mlr.press/v5/li09d/li09d.pdf}{\underline{\textcolor{blue}{this paper by Li, Szepesvari, Schuurmans}}} where we customize the LSPI algorithm to fit the specific nuances of this Optimal Exercise problem. Finally, we show to solve this problem with Deep Q-Learning and Experience Replay.

\section{Review of LSPI}

Let us first consider the general LSPI algorithm. In each iteration, we are given:

$$Q(s,a) = \sum_i w_i \cdot \phi_i(s,a)$$

and deterministic policy $\pi$ (known as the target policy for that iteration) is given by:

$$\pi(s) = \argmax_a Q(s,a)$$

The goal in the iteration is to solve for weights $\{w_i'\}$ such that we minimize

$$\sum_{s,a,r,s'} (r + \gamma \cdot Q'(s',\pi(s')) - Q'(s,a))^2$$

over a data-set comprising of a sequence of 4-tuples $(s,a,r,s')$ with:

$$Q'(s,a) = \sum_i w_i' \cdot \phi_i(s,a)$$
$$Q'(s',\pi(s')) = \sum_i w_i' \cdot \phi_i(s', \pi(s'))$$

Therefore, we solve for $\{w_i'\}$ that minimizes:

$$\sum_{s,a,r,s'} (r + \gamma \cdot Q'(s',\pi(s')) - \sum_i w_i' \cdot \phi_i(s,a))^2$$

So we calculate the gradient of the above expression with respect to $\{w_j'\}$ and set it to 0 (semi-gradient). This gives:

\begin{equation}
\sum_{s,a,r,s'} (r + \gamma \cdot Q'(s',\pi(s')) - \sum_i w_i' \cdot \phi_i(s,a)) \cdot \phi_j(s,a) = 0 \text{ for all } j
\label{eq:general-lspi}
\end{equation}

\section{LSPI Customization}

Now we customize LSPI to the problem of Optimal Exercise of American Options. We have two actions: $a=c$ (continue the American Option) and $a=e$ (exercise the American Option). We consider a function approximation for $Q'(s,a)$ only for the case of $a=c$ since we know the exact expression for the case of $a=e$, given by the option payoff function (call it $g: \mathcal{S} \rightarrow \mathbb{R}$). Therefore,

$$Q'(s,e) = g(s)$$

We write the function approximation for $Q'(s,c)$ as:

$$Q'(s,c) = \sum_i w_i' \cdot \phi_i(s, c) = \sum_i w_i' \cdot x_i(s)$$

for feature functions $x_i: \mathcal{S} \rightarrow \mathbb{R}$ (i.e., functions of only state and not action).

Since we are learning the Q-Value function for only $a=c$, our experience policy $\mu$ is a constant function $\mu(s) = c$. Also, for American Options, the reward for $a=c$ is 0. Thus, when considering the 4-tuples $(s,a,r,s')$ for training experience, we always have $a=c$ and $r=0$. So the 4-tuples are $(s,c,0,s')$ and so, we might as well simply consider 2-tuples $(s,s')$ for training experience (since $a=c$ and $r=0$ are locked).

Now consider 2 cases to customize Equation \eqref{eq:general-lspi} to the problem of Optimal Exercise of American Options.

{\bf Case 1}: If $s'$ is a non-terminal state and $\pi(s') = c$ (this happens when $\sum_i w_i \cdot x_i(s') \geq g(s')$), then Equation \eqref{eq:general-lspi} reduces to:

$$\sum_{s,s'} (\gamma \cdot \sum_i w_i' \cdot x_i(s') - \sum_i w_i' \cdot x_i(s)) \cdot x_j(s) = 0 \text{ for all } j$$
\begin{equation}
\Rightarrow \sum_i w_i' \cdot \sum_{s,s'} x_j(s) \cdot (x_i(s) - \gamma \cdot x_i(s')) = \sum_{s,s'} 0 \text{ for all } j
\label{eq:lspi-amopt1}
\end{equation}

{\bf Case 2}: If $s'$ is a terminal state or $\pi(s') = e$ (this happens when $g(s') > \sum_i w_i \cdot x_i(s')$), then Equation \eqref{eq:general-lspi} reduces to:

$$\sum_{s,s'} (\gamma \cdot g(s') - \sum_i w_i' \cdot x_i(s)) \cdot x_j(s) = 0 \text{ for all } j$$
\begin{equation}
\Rightarrow \sum_i w_i' \cdot \sum_{s,s'} x_j(s) \cdot (x_i(s)) = \sum_{s,s'} x_j(s) \cdot \gamma \cdot g(s') \text{ for all } j
\label{eq:lspi-amopt2}
\end{equation}

Equations \eqref{eq:lspi-amopt1} and \eqref{eq:lspi-amopt2} can be united with a common equation ${\bf A} \cdot {\bf w'} = {\bf b}$.
The term $x_j(s) \cdot (x_i(s) - \gamma \cdot x_i(s'))$ from Equation \eqref{eq:lspi-amopt1} (for Case 1) and the term $x_j(s) \cdot (x_i(s))$ from Equation \eqref{eq:lspi-amopt2} (for Case 2) contributes to the ${\bf A}$ matrix for each $(s,s')$ in the training experience data-set. The term 0 from Equation \eqref{eq:lspi-amopt1} (for Case 1) and the term $x_j(s) \cdot \gamma \cdot g(s')$ from Equation \eqref{eq:lspi-amopt2} (for Case 2) contributes to the ${\bf b}$ vector for each $(s,s')$ in the training experience data-set.

\section{Solving with Deep Q-Learning and Experience Replay}

Although the above LSPI algorithm is data-efficient and computationally-efficient as well, it is limited by the fact that our function approximation needs to be linear in features. Linearity is a significant constraint on this problem. In order to use a more general function approximation (such as a deep neural network), we will need to look into traditional incremental RL algorithms. A straightforward choice is Q-Learning. We will employ Q-Learning together with Experience Replay by storing 2-tuples of $(s,s')$ in a buffer and drawing from the buffer randomly (the random draws serve as training experience).

We use the same notation as we used above for LSPI. Let $\hat{f}$ denote a deep neural network function approximation for the Q-Value function when the action is to continue. Therefore,

$$Q(s,c) = \hat{f}(s;w)$$
$$Q(s,a) = g(s)$$
$$
\pi(s) =
\begin{cases}
c & \text{ if } \hat{f}(s;w) \geq g(s) \\
e & \text{ otherwise}
\end{cases}
$$

where $w$ denotes the weights of the deep neural network and $g(\cdot)$ is the option payoff function.

Noting that in the experience 4-tuples $(s,a,r,s')$, $a=c$ and $r=0$ (as explained earlier), the Q-Learning update for each 2-tuple $(s,s)$ from the Experience Replay is as follows:

$$w \leftarrow w + \alpha \cdot (\gamma \cdot Q(s', \pi(s')) - \hat{f}(s;w)) \cdot \nabla_w \hat{f}(s;w)$$

where $\alpha$ is the learning rate.

When $s'$ is a non-terminal state, the update is:

$$w \leftarrow w + \alpha \cdot (\gamma \cdot \max(g(s'), \hat{f}(s';w)) - \hat{f}(s;w)) \cdot \nabla_w \hat{f}(s;w)$$

When $s'$ is a terminal state, the update is:

$$w \leftarrow w + \alpha \cdot (\gamma \cdot g(s') - \hat{f}(s;w)) \cdot \nabla_w \hat{f}(s;w)$$
 
\end{document}