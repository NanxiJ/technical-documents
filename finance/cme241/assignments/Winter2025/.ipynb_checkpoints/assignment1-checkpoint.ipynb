{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2025) - Assignment 1\n",
    "\n",
    "**Due: Sunday, January 19 @ 11:59 PM PST on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- Make sure each of the subquestions have answers\n",
    "- Ensure that group members indicate which problems they're in charge of\n",
    "- Show work and walk through your thought process where applicable\n",
    "- Empty code blocks are for your use, so feel free to create more under each section as needed\n",
    "- Document code with light comments (i.e. 'this function handles visualization')\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/my-username/my-repo/assignment-file-name.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "- Person 1\n",
    "- Person 2\n",
    "- Person 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Snakes and Ladders (Led by ______)\n",
    "\n",
    "In the classic childhood game of Snakes and Ladders, all players start to the left of square 1 (call this position 0) and roll a 6-sided die to represent the number of squares they can move forward. The goal is to reach square 100 as quickly as possible. Landing on the bottom rung of a ladder allows for an automatic free-pass to climb, e.g. square 4 sends you directly to 14; whereas landing on a snake's head forces one to slide all the way to the tail, e.g. square 34 sends you to 6. Note, this game can be viewed as a Markov Process, where the outcome is only depedent on the current state and not the prior trajectory. In this question, we will ask you to both formally describe the Markov Process that describes this game, followed by coding up a version of the game to get familiar with the RL-book libraries.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "How can we model this problem with a Markov Process?\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): MDP Modeling\n",
    "\n",
    "Formalize the state space of the Snakes and Ladders game. Don't forget to specify the terminal state!\n",
    "\n",
    "#### Part (B): Transition Probabilities\n",
    "\n",
    "Write out the structure of the transition probabilities. Feel free to abbreviate all squares that do not have a snake or ladder.\n",
    "\n",
    "#### Part (C): Modeling the Game\n",
    "\n",
    "Code up a `transition_map: Transition[S]` data structure to represent the transition probabilities of the Snakes and Ladders Markov Process so you can model the game as an instance of `FiniteMarkovProcess`. Use the `traces` method to create sampling traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "#### State Space:\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "#### Transition Probabilities:\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in with Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (Led by ______)\n",
    "\n",
    "Consider an MDP with an infinite set of states $\\mathcal{S} = \\{1,2,3,\\ldots \\}$. The start state is $s=1$. Each state $s$ allows a continuous set of actions $a \\in [0,1]$. The transition probabilities are given by: \n",
    "$$\\mathbb{P}[s+1 \\mid s, a] = a, \\mathbb{P}[s \\mid s, a] = 1 - a \\text{ for all } s \\in \\mathcal{S} \\text{ for all } a \\in [0,1]$$\n",
    "For all states $s \\in \\mathcal{S}$ and actions $a \\in [0,1]$, transitioning from $s$ to $s+1$ results in a reward of $1-a$ and transitioning from $s$ to $s$ results in a reward of $1+a$. The discount factor $\\gamma=0.5$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "How can we derive a mathematical formulation for the value function and the optimal policy? And how do those functions change when we modify the action space?\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): Optimal Value Function  \n",
    "\n",
    "Using the MDP Bellman Optimality Equation, calculate the Optimal Value Function $V^*(s)$ for all $s \\in \\mathcal{S}$.\n",
    "\n",
    "#### Part (B): Optimal Policy  \n",
    "\n",
    "Calculate an Optimal Deterministic Policy $\\pi^*(s)$ for all $s \\in \\mathcal{S}$.\n",
    "\n",
    "#### Part (C): Changing the Action Space  \n",
    "\n",
    "Let's assume that we modify the action space such that instead of $a \\in [0,1]$ for all states, we restrict the action space to $a \\in \\left[0,\\frac{1}{s}\\right]$ for state $s$. This means that higher states have more restricted action spaces. How does this constraint affect:\n",
    "\n",
    "- The form of the Bellman optimality equation?\n",
    "\n",
    "- The optimal value function, $V^*(s)$?\n",
    "\n",
    "- The structure of the optimal policy, $\\pi^*(s)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "#### Optimal Value Function:\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "#### Optimal Deterministic Policy:\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "#### Bellman Optimality Equation Change:\n",
    "<span style=\"color:red\">*fill in*</span>\n",
    "\n",
    "#### Optimal Value Function Change:\n",
    "<span style=\"color:red\">*fill in*</span>\n",
    "\n",
    "#### Optimal Policy Change:\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Frog in a Pond (Led by ______)\n",
    "\n",
    "Consider an array of $n+1$ lilypads on a pond, numbered $0$ to $n$. A frog sits on a lilypad other than the lilypads numbered $0$ or $n$. When on lilypad $i$ ($1 \\leq i \\leq n-1$), the frog can croak one of two sounds: **A** or **B**. \n",
    "\n",
    "- If it croaks **A** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
    "  - It is thrown to lilypad $i-1$ with probability $\\frac{i}{n}$.\n",
    "  - It is thrown to lilypad $i+1$ with probability $\\frac{n-i}{n}$.\n",
    "  \n",
    "- If it croaks **B** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
    "  - It is thrown to one of the lilypads $0, \\ldots, i-1, i+1, \\ldots, n$ with uniform probability $\\frac{1}{n}$.\n",
    "\n",
    "A snake, perched on lilypad $0$, will eat the frog if it lands on lilypad $0$. The frog can escape the pond (and hence, escape the snake!) if it lands on lilypad $n$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "What should the frog croak when on each of the lilypads $1, 2, \\ldots, n-1$, in order to maximize the probability of escaping the pond (i.e., reaching lilypad $n$ before reaching lilypad $0$)? \n",
    "\n",
    "Although there are multiple ways to solve this problem, we aim to solve it by modeling it as a **Markov Decision Process (MDP)** and identifying the **Optimal Policy**.\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): MDP Modeling\n",
    "\n",
    "Express the frog-escape problem as an MDP by clearly defining the following components: \n",
    "\n",
    "- **State Space**: Define the possible states of the MDP. \n",
    "\n",
    "- **Action Space**: Specify the actions available to the frog at each state. \n",
    "\n",
    "- **Transition Function**: Describe the probabilities of transitioning between states for each action. \n",
    "\n",
    "- **Reward Function**: Specify the reward associated with the states and transitions. \n",
    "\n",
    "#### Part (B): Python Implementation\n",
    "\n",
    "Write Python code that:\n",
    "\n",
    "- Models this MDP.\n",
    "\n",
    "- Solves the **Optimal Value Function** and the **Optimal Policy**.\n",
    "\n",
    "Feel free to use/adapt code from the textbook.\n",
    "\n",
    "#### Part (C): Visualization and Analysis\n",
    "\n",
    "Using your Python code,\n",
    "\n",
    "- Plot a graph of the **Optimal Escape-Probability** and the associated **Optimal Croak** as a function of the states of the MDP for $n=3$, $n=10$, and $n=25$.\n",
    "- Include these graphs in your submission.\n",
    "\n",
    "#### Part (D): Observations\n",
    "\n",
    "- What patterns do you observe for the **Optimal Policy** as you vary $n$ from 3 to 25?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "#### State Space:  \n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>\n",
    "\n",
    "#### Action Space:  \n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>\n",
    "\n",
    "#### Transition Function:  \n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>\n",
    "\n",
    "#### Reward Function:  \n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in with Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in with Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observed Patterns:\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
