\documentclass[12pt]{exam}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} % Allows including images
\usepackage{cool}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{pseudocode}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{MnSymbol,wasysym}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} 
\newgeometry{vmargin={20mm}, hmargin={14mm,18mm}}
 
\begin{document}
\begin{center}
{\large {\bf Stanford CME 241 (Winter 2022) - Assignment 12}}
\end{center}
 
{\large{\bf Assignments:}}
\begin{questions}
\question {\bf Optional} Implement the $n$-Step Bootstrapping Prediction algorithm from scratch in Python code. First do it for the Tabular case, then do it for the case of Function Approximation.
\question Implement the TD($\lambda$) Prediction algorithm from scratch in Python code. First do it for the Tabular case, then do it for the case of Function Approximation.
\question Prove that the MC Error can be written as the sum of discounted TD errors, i.e.,
$$G_t - V(S_t) = \sum_{u=t}^{T-1} \gamma^{u-t} \cdot (R_{u+1} + \gamma \cdot V(S_{u+1}) - V(S_u))$$
The goal here is for you to practice formal proof-writing of these types of simple yet important identities. So aim to work this out from scratch rather than treating this as a special case of a more general result proved in class or in the textbook.
\question Test your above implementation of TD($\lambda$) Prediction algorithm by comparing the Value Function of an MRP you have previously developed (or worked with) as obtained by Policy Evaluation (DP) algorithm, as obtained by MC, as obtained by TD, and as obtained by your TD($\lambda$) implementation. Plot graphs of convergence for different values of $\lambda$.
\end{questions}

\end{document}