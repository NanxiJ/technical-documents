\documentclass[12pt]{exam}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} % Allows including images
\usepackage{cool}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{pseudocode}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{MnSymbol,wasysym}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} 
\newgeometry{vmargin={20mm}, hmargin={14mm,18mm}}
 
\begin{document}
\begin{center}
{\large {\bf Stanford CME 241 (Winter 2022) - Assignment 5}}
\end{center}
 
{\large{\bf Assignments:}}
\begin{questions}
	\question Implement another form of \lstinline{FunctionApprox} beyond what we've covered in class (we covered linear function approximation, deep neural networks, and tabular). If you have taken a course in statistical learning, you can try implementing \href{https://en.wikipedia.org/wiki/Generalized_additive_model}{Generalized Additive Model (GAM)}. Otherwise, try implementing \href{https://en.wikipedia.org/wiki/Spline_(mathematics)}{Univariate B-Spline}. Be sure to test your implementation on an appropriate data set - you can generate a data set from a model (from either the same model you are implementing or a similar/related model).
	\question {\bf Optional:} Implement {\em Approximate Policy Iteration}, generalization of the tabular Policy Iteration we covered in the previous class. In order to implement Approximate Policy Iteration, first review the interface and implementation of {\em Approximate Policy Evaluation} and {\em Approximate Value Iteration} (in file \href{https://github.com/TikhonJelvis/RL-book/blob/master/rl/approximate_dynamic_programming.py}{rl\//approximate\_dynamic\_programming.py}), then design the interface of {\em Approximate Policy Iteration} to be the same as that of {\em Approximate Value Iteration}. Note that your implementation of {\em Approximate Policy Iteration} would need to invoke {\em Approximate Policy Evaluation} since Policy Evaluation is a component of Policy Iteration. Test that your implementation is correct in two ways:
	\begin{itemize}
	\item Ensure that {\em Approximate Policy Iteration} gives the same Optimal Value Function/Optimal Policy as that obtained by {\em Approximate Value Iteration}.
	\item Ensure that {\em Approximate Policy Iteration} produces the same result as our prior implementation of Policy Iteration (in file \href{https://github.com/TikhonJelvis/RL-book/blob/master/rl/dynamic_programming.py}{rl\//dynamic\_programming.py}). For this you need to pass to your implementation of {\em Approximate Policy Iteration} a \lstinline{FiniteMarkovDecisionProcess} input and a \lstinline{Tabular} instance for the \lstinline{FunctionApprox} input.
	\end{itemize}
\end{questions}

\end{document}
