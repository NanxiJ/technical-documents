\documentclass[12pt]{exam}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} % Allows including images
\usepackage{cool}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{pseudocode}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{MnSymbol,wasysym}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} 
\newgeometry{vmargin={20mm}, hmargin={14mm,18mm}}
 
\begin{document}
\begin{center}
{\large {\bf Stanford CME 241 (Winter 2021) - Assignment 7}}
\end{center}
 
{\large{\bf Assignments:}}
\begin{questions}
\question Derive the solution to Merton's Portfolio problem for the case of the $\log(\cdot)$ Utility function. Note that the derivation in the textbook is for CRRA Utility function with $\gamma \neq 1$ and the case of the $\log(\cdot)$ Utility function was left as an exercise to the reader.
\question {\bf Optional:} One of the reasons the backward induction solution in \href{https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter7/asset_alloc_discrete.py}{rl\//chapter7\//asset\_alloc\_discrete.py} is slow is that we work with a generic \lstinline{Distribution} type for \lstinline{risky_return_distributions}, which means we have to sequentially sample from it to create the states distribution (in method \lstinline{get_states_distribution}) that can be passed as input to \lstinline{back_opt_qvf}. Modify the code to create a special type of distribution for the returns of the risky asset so we have a direct way of obtaining the probability distribution of the risky asset price at any time step (and hence, the probability distribution of wealth at any time step). With a direct way to obtain probability distribution of states at any time step, we can speed up the code considerably. 
\question Sketch out a rough design for the following MDP:
\begin{itemize}
\item Your job is such that you can spend a fraction $\alpha$ of each day working and the remaining fraction $1-\alpha$ of your day learning.
\item Each minute that you spend in a day working earns you at the rate of $f(s)$ dollars per minutes where $s$ is your current skill level.
\item Each minute you spend on learning improves your skill level by a certain factor $g(s)$ where $s$ is your current skill level.
\item You could lose your job any day with a probability $p$.
\item While unemployed, you do not have access to learning, so your skill level decays exponentially with half life $\lambda$.
\item If you've lost your job, you could be offered the job back with a probability $h(s)$ where $s$ is your current skill level.
\end{itemize}

What should be your optimal policy for fraction of time on learning on any given day, if your goal is to maximize your Expected (Discounted) Lifetime Utility of Earnings?
Think about variants such as finite-horizon and infinite-horizon. What if there were multiple skills or multiple job options? You can also factor in the fact that you need to consume your earnings, and so consumption quantity on any given day will also be an action, and then the objective would be Expected (Discounted) Lifetime Utility of Consumption.

This question is open-ended and is meant as a thought exercise. But do try to sketch out the states, actions, rewards and transition function.

\end{questions}

\end{document}