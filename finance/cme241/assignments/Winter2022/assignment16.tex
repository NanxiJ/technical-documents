\documentclass[12pt]{exam}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} % Allows including images
\usepackage{cool}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{pseudocode}
\usepackage{bm}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{MnSymbol,wasysym}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} 
\newgeometry{vmargin={20mm}, hmargin={14mm,18mm}}
 
\begin{document}
\begin{center}
{\large {\bf Stanford CME 241 (Winter 2021) - Assignment 16}}
\end{center}
 
{\large{\bf Assignments:}}
\begin{questions}
\question {\bf Optional} Implement the Monte-Carlo Policy Gradient (REINFORCE) algorithm in Python and test it by checking that you recover the closed-form solution of the Discrete-Time Asset-Allocation example (single risky asset with no consumption before terminal date). The lecture slides have the pseudo-code for this algorithm.
\question {\bf Optional} Implement the ACTOR-CRITIC-ELIGIBILITY-TRACES Policy Gradient algorithm in Python and test it by checking that you recover the closed-form solution of the Discrete-Time Asset-Allocation example (single risky asset with no consumption before terminal date). The lecture slides have the pseudo-code for this algorithm.
\question Assume we have a finite action space $\mathcal{A}$. Let $\bm{\phi}(s,a) = (\phi_1(s,a), \phi_2(s,a), \ldots, \phi_m(s,a))$ be the features vector for any $s \in \mathcal{N}, a \in \mathcal{A}$. Let $\bm{\theta} = (\theta_1, \theta_2, \ldots, \theta_m)$ be an $m$-vector of parameters. Let the action probabilities conditional on a given state $s$ and given parameter vector $\bm{\theta}$ be defined by the softmax function on the linear combination of features: $\bm{\phi}(s,a)^T \cdot \bm{\theta}$, i.e.,
$$\pi(s, a ; \bm{\theta}) = \frac {e^{\bm{\phi}(s,a)^T \cdot \bm{\theta}}} {\sum_{b \in \mathcal{A}} e^{\bm{\phi}(s,b)^T \cdot \bm{\theta}}}$$
\begin{itemize}
\item Evaluate the score function $\nabla_{\bm{\theta}} \log \pi(s, a ; \bm{\theta})$
\item Construct the Action-Value function approximation $Q(s,a; \bm{w})$ so that the following key constraint of the Compatible Function Approximation Theorem (for Policy Gradient) is satisfied:
 $$\nabla_{\bm{w}} Q(s,a;\bm{w}) = \nabla_{\bm{\theta}} \log \pi(s, a ; \bm{\theta})$$
 where $\bm{w}$ defines the parameters of the function approximation of the Action-Value function.
 \item Show that $Q(s,a ; \bm{w})$ has zero mean for any state $s$, i.e. show that
 $$\mathbb{E}_{\pi} [Q(s,a; \bm{w})] \mbox{ defined as } \sum_{a \in \mathcal{A}} \pi(s, a ; \bm{\theta}) \cdot Q(s,a ; \bm{w}) = 0 \text{ for all } s \in \mathcal{N}$$
\end{itemize}

\end{questions}

\end{document}