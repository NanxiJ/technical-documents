{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2024) - Assignment 4\n",
    "\n",
    "**Due: Feb 5 @ 11:59pm Pacific Time on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- **Please solve questions 1 and 2, and choose one of questions 3 or 4.**\n",
    "- Empty code blocks are for your use. Feel free to create more under each section as needed.\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/my-username/my-repo/assignment-file-name.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "- Person 1\n",
    "- Person 2\n",
    "- Person 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Implement *Approximate Policy Iteration*, generalization of the tabular\n",
    "Policy Iteration we covered in the previous class. In order to implement\n",
    "Approximate Policy Iteration, first review the interface and\n",
    "implementation of *Approximate Policy Evaluation* and *Approximate Value\n",
    "Iteration* (in file\n",
    "[rl/approximate_dynamic_programming.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/approximate_dynamic_programming.py)),\n",
    "then design the interface of *Approximate Policy Iteration* to be the\n",
    "same as that of *Approximate Value Iteration*. Note that your\n",
    "implementation of *Approximate Policy Iteration* would need to invoke\n",
    "*Approximate Policy Evaluation* since Policy Evaluation is a component\n",
    "of Policy Iteration. Test that your implementation is correct in two\n",
    "ways:\n",
    "\n",
    "-   Ensure that *Approximate Policy Iteration* gives the same Optimal\n",
    "    Value Function/Optimal Policy as that obtained by *Approximate Value\n",
    "    Iteration*.\n",
    "\n",
    "-   Ensure that *Approximate Policy Iteration* produces the same result\n",
    "    as our prior implementation of Policy Iteration (in file\n",
    "    [rl/dynamic_programming.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/dynamic_programming.py)).\n",
    "    For this you need to pass to your implementation of *Approximate\n",
    "    Policy Iteration* a `FiniteMarkovDecisionProcess` input and a\n",
    "    `Tabular` instance for the `FunctionApprox` input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Assume the Utility function is $U(x) = x - \\frac {\\alpha x^2} 2$.\n",
    "Assuming $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$, calculate:\n",
    "\n",
    "-   Expected Utility $\\mathbb{E}[U(x)]$\n",
    "\n",
    "-   Certainty-Equivalent Value $x_{CE}$\n",
    "\n",
    "-   Absolute Risk-Premium $\\pi_A$\n",
    "\n",
    "\n",
    "Assume you have a million dollars to invest for a year and you are\n",
    "allowed to invest $z$ dollars in a risky asset whose annual return on\n",
    "investment is $\\mathcal{N}(\\mu, \\sigma^2)$ and the remaining (a million\n",
    "minus $z$ dollars) would need to be invested in a riskless asset with\n",
    "fixed annual return on investment of $r$. You are not allowed to adjust\n",
    "the quantities invested in the risky and riskless assets after your\n",
    "initial investment decision at time $t=0$ (static asset allocation\n",
    "problem). If your risk-aversion is based on this Utility function, how\n",
    "much would you invest in the risky asset? In other words, what is the\n",
    "optimal value for $z$, given your level of risk-aversion (determined by\n",
    "a fixed value of $\\alpha$)?\n",
    "\n",
    "Plot how the optimal value of $z$ varies with $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Assume you are playing a casino game where at every turn, if you bet a\n",
    "quantity $x$, you will be returned $x \\cdot (1 + \\alpha)$ with\n",
    "probability $p$ and returned $x \\cdot (1 - \\beta)$ with probability\n",
    "$q = 1 - p$ for $\\alpha, \\beta \\in \\mathbb{R}^+$ (i.e., the return on\n",
    "bet is $\\alpha$ with probability $p$ and $-\\beta$ with probability\n",
    "$q = 1-p$) . The problem is to identify a betting strategy that will\n",
    "maximize one's expected wealth over the long run. The optimal solution\n",
    "to this problem is known as the Kelly criterion, which involves betting\n",
    "a constant fraction of one's wealth at each turn (let us denote this\n",
    "optimal fraction as $f^*$).\n",
    "\n",
    "It is known that the Kelly criterion (formula for $f^*$) is equivalent\n",
    "to maximizing the Expected Utility of Wealth after a single bet, with\n",
    "the Utility function defined as: $U(W) = \\log(W)$. Denote your wealth\n",
    "before placing the single bet as $W_0$. Let $f$ be the fraction (to be\n",
    "solved for) of $W_0$ that you will bet. Therefore, your bet is\n",
    "$f \\cdot W_0$.\n",
    "\n",
    "-   Write down the two outcomes for wealth $W$ at the end of your single\n",
    "    bet of $f \\cdot W_0$.\n",
    "\n",
    "-   Write down the two outcomes for $\\log$ (Utility) of $W$.\n",
    "\n",
    "-   Write down $\\mathbb{E}[\\log(W)]$.\n",
    "\n",
    "-   Take the derivative of $\\mathbb{E}[\\log(W)]$ with respect to $f$.\n",
    "\n",
    "-   Set this derivative to 0 to solve for $f^*$. Verify that this is\n",
    "    indeed a maxima by evaluating the second derivative at $f^*$. This\n",
    "    formula for $f^*$ is known as the Kelly Criterion.\n",
    "\n",
    "-   Convince yourself that this formula for $f^*$ makes intuitive sense\n",
    "    (in terms of it's dependency on $\\alpha$, $\\beta$ and $p$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Derive the solution to Merton's Portfolio problem for the case of the\n",
    "$\\log(\\cdot)$ Utility function. Note that the derivation in the textbook\n",
    "is for CRRA Utility function with $\\gamma \\neq 1$ and the case of the\n",
    "$\\log(\\cdot)$ Utility function was left as an exercise to the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
